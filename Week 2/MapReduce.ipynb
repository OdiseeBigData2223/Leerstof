{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Map Reduce\n",
    "\n",
    "Eerst maken we een aparte directory aan voor alles wat we voor deze notebook gaan gebruiken in hdfs. Dit om conflicten of het overschrijven van gegevens te vermijden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pydoop.hdfs as hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 14:54:39,565 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user/bigdata/MapReduce\n"
     ]
    }
   ],
   "source": [
    "# make links with local and hdfs file systems\n",
    "localFS = hdfs.hdfs(host='')\n",
    "client = hdfs.hdfs(host='localhost', port=9000)\n",
    "\n",
    "# if directory does not exists - make directory\n",
    "if not client.exists('/user/bigdata/MapReduce'):\n",
    "    client.create_directory('/user/bigdata/MapReduce')\n",
    "client.set_working_directory('/user/bigdata/MapReduce')\n",
    "print(client.working_directory())\n",
    "\n",
    "# do some cleaning in case anything else than input.txt is present\n",
    "for f in client.list_directory(\".\"):\n",
    "    if not f[\"name\"].endswith(\"input.txt\"):\n",
    "        client.delete(f[\"name\"], True) # True om aan te geven dat folders ook verwijderd moeten worden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Wat is MapReduce\n",
    "\n",
    "MapReduce is een programmeermodel om eenvoudig distributed data te verwerken.\n",
    "Het is belangrijk om te realiseren dat de programma's die je hier schrijft een parallel uitgevoerd worden op verschillende stukjes data (De map-fase) om daarna in de reduce-fase tot een finale output teruggebracht te worden.\n",
    "Er gebeuren 5 stappen bij het uitvoeren van een MapReduce programma\n",
    "* Bepalen op welke nodes de code uitgevoerd wordt (wordt door YARN gedaan afhankelijk van de locatie van de blocks)\n",
    "* Uitvoeren van de Map-code (Geschreven door de developer naar eigen wens)\n",
    "* Shuffle, ouput van de map-fase doorsturen naar andere nodes die de resultaten gaan reduceren (Automatisch)\n",
    "* uitvoeren van de Reduce-code (Geschreven door de developer naar eigen wens)\n",
    "* Combineren van de reduce output tot 1 gehele/finale output (Automatisch)\n",
    "\n",
    "Uit bovenstaand stappenplan is het duidelijk dat er twee zaken moeten geimplementeerd worden bij het schrijven van een MapReduce toepassing.\n",
    "Echter zullen we eerst een aantal voorbeelden bestuderen met reeds bestaande implementaties om zo meer vertrouwd te geraken met de flow van MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Voorbeelden van bestaande applicaties\n",
    "\n",
    "Reeds een aantal default MapReduce applications zijn mee geinstalleerd met Hadoop.\n",
    "De jar die deze toepassingen bevat kan gevonden worden in hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar.\n",
    "Wanneer je deze jar uitvoert met onderstaande commando krijg je een lijst met de beschikbare applicaties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example program must be given as the first argument.\r\n",
      "Valid program names are:\r\n",
      "  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.\r\n",
      "  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.\r\n",
      "  bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.\r\n",
      "  dbcount: An example job that count the pageview counts from a database.\r\n",
      "  distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.\r\n",
      "  grep: A map/reduce program that counts the matches of a regex in the input.\r\n",
      "  join: A job that effects a join over sorted, equally partitioned datasets\r\n",
      "  multifilewc: A job that counts words from several files.\r\n",
      "  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.\r\n",
      "  pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.\r\n",
      "  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.\r\n",
      "  randomwriter: A map/reduce program that writes 10GB of random data per node.\r\n",
      "  secondarysort: An example defining a secondary sort to the reduce.\r\n",
      "  sort: A map/reduce program that sorts the data written by the random writer.\r\n",
      "  sudoku: A sudoku solver.\r\n",
      "  teragen: Generate data for the terasort\r\n",
      "  terasort: Run the terasort\r\n",
      "  teravalidate: Checking results of terasort\r\n",
      "  wordcount: A map/reduce program that counts the words in the input files.\r\n",
      "  wordmean: A map/reduce program that counts the average length of the words in the input files.\r\n",
      "  wordmedian: A map/reduce program that counts the median length of the words in the input files.\r\n",
      "  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar ~/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In deze notebook gaan we vooral focussen op het typische probleem van wordcount.\n",
    "Dit is een toepassing dat gaat tellen hoe vaak elk woord voorkomt in een bepaalde tekst.\n",
    "Om meer informatie over deze toepassing te krijgen kan je gebruik maken van het volgende commando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: wordcount <in> [<in>...] <out>\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar ~/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Voor we deze applicatie kunnen uitvoeren moeten we eerst ervoor zorgen dat er input data beschikbaar is in HDFS.\n",
    "Upload hiervoor de input.txt file met onderstaande code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "localFS.copy(\"input.txt\", client, \"input.txt\") # upload file to hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Het toevoegen van dit bestand kan geverifieerd worden door het HDFS te gaan bekijken op volgende link [http://localhost:9870/explorer.html#/user/bigdata/05_MapReduce](http://localhost:9870/explorer.html#/user/bigdata/05_MapReduce)\n",
    "\n",
    "Met onderstaande commando kan nu het precompiled word count example uitgevoerd worden\n",
    "\n",
    "**LET OP:** Paden naar de files in de terminal worden steeds als in de terminal uitgevoerd met user bigdata. Hierdoor begint ons pad standaard op /user/bigdata/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-16 14:59:49,178 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2023-02-16 14:59:50,952 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/bigdata/.staging/job_1675953405945_0001\n",
      "2023-02-16 14:59:52,004 INFO input.FileInputFormat: Total input files to process : 1\n",
      "2023-02-16 14:59:52,841 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2023-02-16 14:59:55,089 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1675953405945_0001\n",
      "2023-02-16 14:59:55,090 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-02-16 14:59:56,418 INFO conf.Configuration: found resource resource-types.xml at file:/home/bigdata/hadoop/etc/hadoop/resource-types.xml\n",
      "2023-02-16 14:59:56,506 INFO resource.ResourceUtils: Adding resource type - name = vram, units = G, type = COUNTABLE\n",
      "2023-02-16 14:59:58,247 INFO impl.YarnClientImpl: Submitted application application_1675953405945_0001\n",
      "2023-02-16 14:59:58,411 INFO mapreduce.Job: The url to track the job: http://bigdata-VirtualBox:8088/proxy/application_1675953405945_0001/\n",
      "2023-02-16 14:59:58,427 INFO mapreduce.Job: Running job: job_1675953405945_0001\n",
      "2023-02-16 15:00:47,195 INFO mapreduce.Job: Job job_1675953405945_0001 running in uber mode : false\n",
      "2023-02-16 15:00:47,196 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-02-16 15:01:11,836 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-02-16 15:01:28,249 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-02-16 15:01:30,395 INFO mapreduce.Job: Job job_1675953405945_0001 completed successfully\n",
      "2023-02-16 15:01:31,177 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=177\n",
      "\t\tFILE: Number of bytes written=532407\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=223\n",
      "\t\tHDFS: Number of bytes written=111\n",
      "\t\tHDFS: Number of read operations=8\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=22214\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=13749\n",
      "\t\tTotal time spent by all map tasks (ms)=22214\n",
      "\t\tTotal time spent by all reduce tasks (ms)=13749\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=22214\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=13749\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=22747136\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=14078976\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5\n",
      "\t\tMap output records=18\n",
      "\t\tMap output bytes=176\n",
      "\t\tMap output materialized bytes=177\n",
      "\t\tInput split bytes=119\n",
      "\t\tCombine input records=18\n",
      "\t\tCombine output records=15\n",
      "\t\tReduce input groups=15\n",
      "\t\tReduce shuffle bytes=177\n",
      "\t\tReduce input records=15\n",
      "\t\tReduce output records=15\n",
      "\t\tSpilled Records=30\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=263\n",
      "\t\tCPU time spent (ms)=4480\n",
      "\t\tPhysical memory (bytes) snapshot=426508288\n",
      "\t\tVirtual memory (bytes) snapshot=5299716096\n",
      "\t\tTotal committed heap usage (bytes)=360452096\n",
      "\t\tPeak Map Physical memory (bytes)=264380416\n",
      "\t\tPeak Map Virtual memory (bytes)=2645438464\n",
      "\t\tPeak Reduce Physical memory (bytes)=162127872\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2654277632\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=104\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=111\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar ~/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar wordcount MapReduce/input.txt MapReduce/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Indien het bovenstaande commando een foutmelding geeft over het niet vinden van org.apache.hadoop.mapreduce.v2.app.MRAppMaster, voeg het volgende toe aan hadoop/etc/hadoop/mapred-site.xml\n",
    "\n",
    "    <property>\n",
    "\t  \t<name>yarn.app.mapreduce.am.env</name>\n",
    "\t  \t<value>HADOOP_MAPRED_HOME=/home/bigdata/hadooop</value>\n",
    "\t</property>\n",
    "\t<property>\n",
    "\t  <name>mapreduce.map.env</name>\n",
    "\t  <value>HADOOP_MAPRED_HOME=/home/bigdata/hadooop</value>\n",
    "\t</property>\n",
    "\t<property>\n",
    "\t  <name>mapreduce.reduce.env</name>\n",
    "\t  <value>HADOOP_MAPRED_HOME=/home/bigdata/hadooop</value>\n",
    "\t</property>\n",
    "\n",
    "Na dit toe te voegen, herstart yarn en hdfs.\n",
    "Indien het hier nog niet mee opgelost is voer de volgende stappen uit:\n",
    "* Voer \"hadoop classpath\" uit in de terminal\n",
    "* Voeg het volgende toe aan hadoop/etc/hadoop/yarn-site.xml:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "    <property>\n",
    "        <name>yarn.application.classpath</name>\n",
    "        <value>output van stap 1</value>\n",
    "    </property>\n",
    "\n",
    "Na het uitvoeren van het wordcount applicatie moet er een output.txt file aangemaakt zijn met de resultaten. **Bekijk dit nu.**\n",
    "Het resultaat is een file met key-value pairs met de woorden als key en de frequentie/aantal keer dat de woorden voorkomen als key.\n",
    "\n",
    "Probeer nu op basis van bovenstaande reeds bestaande applicaties de gemiddelde lengte van de woorden in de text te bepalen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-16 15:11:29,151 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2023-02-16 15:11:30,788 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/bigdata/.staging/job_1675953405945_0002\n",
      "2023-02-16 15:11:31,681 INFO input.FileInputFormat: Total input files to process : 1\n",
      "2023-02-16 15:11:31,895 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2023-02-16 15:11:32,472 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1675953405945_0002\n",
      "2023-02-16 15:11:32,472 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-02-16 15:11:32,856 INFO conf.Configuration: found resource resource-types.xml at file:/home/bigdata/hadoop/etc/hadoop/resource-types.xml\n",
      "2023-02-16 15:11:32,935 INFO resource.ResourceUtils: Adding resource type - name = vram, units = G, type = COUNTABLE\n",
      "2023-02-16 15:11:33,121 INFO impl.YarnClientImpl: Submitted application application_1675953405945_0002\n",
      "2023-02-16 15:11:33,178 INFO mapreduce.Job: The url to track the job: http://bigdata-VirtualBox:8088/proxy/application_1675953405945_0002/\n",
      "2023-02-16 15:11:33,182 INFO mapreduce.Job: Running job: job_1675953405945_0002\n",
      "2023-02-16 15:11:52,827 INFO mapreduce.Job: Job job_1675953405945_0002 running in uber mode : false\n",
      "2023-02-16 15:11:52,828 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-02-16 15:12:09,329 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-02-16 15:12:23,678 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-02-16 15:12:24,718 INFO mapreduce.Job: Job job_1675953405945_0002 completed successfully\n",
      "2023-02-16 15:12:25,173 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=39\n",
      "\t\tFILE: Number of bytes written=532155\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=223\n",
      "\t\tHDFS: Number of bytes written=19\n",
      "\t\tHDFS: Number of read operations=8\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=14572\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=10930\n",
      "\t\tTotal time spent by all map tasks (ms)=14572\n",
      "\t\tTotal time spent by all reduce tasks (ms)=10930\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=14572\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=10930\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=14921728\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=11192320\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5\n",
      "\t\tMap output records=36\n",
      "\t\tMap output bytes=522\n",
      "\t\tMap output materialized bytes=39\n",
      "\t\tInput split bytes=119\n",
      "\t\tCombine input records=36\n",
      "\t\tCombine output records=2\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=39\n",
      "\t\tReduce input records=2\n",
      "\t\tReduce output records=2\n",
      "\t\tSpilled Records=4\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=140\n",
      "\t\tCPU time spent (ms)=3300\n",
      "\t\tPhysical memory (bytes) snapshot=429330432\n",
      "\t\tVirtual memory (bytes) snapshot=5296627712\n",
      "\t\tTotal committed heap usage (bytes)=360452096\n",
      "\t\tPeak Map Physical memory (bytes)=264519680\n",
      "\t\tPeak Map Virtual memory (bytes)=2644107264\n",
      "\t\tPeak Reduce Physical memory (bytes)=164810752\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2652520448\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=104\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=19\n",
      "The mean is: 4.777777777777778\n"
     ]
    }
   ],
   "source": [
    "# uit te voeren op commando voor de gemiddelde lengte van de woorden te bekomen\n",
    "!hadoop jar ~/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar wordmean MapReduce/input.txt MapReduce/output_mean_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "De resource manager houdt ook een overzicht bij van de uitgevoerde applicaties, hun status, runtime en eventuele loggings. Na bovenstaande commando's uit te voeren moet je iets gelijkaardigs zien als in de onderstaande screenshot.\n",
    "\n",
    "![yarn of mapreduce](images/yarn_001.png)\n",
    "\n",
    "## Zelf implementeren van MapReduce applicaties\n",
    "\n",
    "Natuurlijk zijn er veel meer zaken mogelijk om te berekenen met map-reduce toepassingen dan de reeds gecompileerde in hadoop.\n",
    "Zoals eerder aangehaald valt vooral het coderen van de Map- en Reducestap hierbij op de schouders van de developer.\n",
    "De standaard programmeertaal van MapReduce is Java en dus ook het grootste deel van de documentatie over MapReduce is geschreven met behulp van Java.\n",
    "Deze programmas moeten dan gecompileerd worden tot een jar dat geupload kan worden naar de overeenkomstige nodes en daar uitgevoerd.\n",
    "De api overview van hadoop kan je [hier](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/package-summary.html) vinden.\n",
    "\n",
    "De code voor het wordcount example ziet er als volgt uit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount.java\n"
     ]
    }
   ],
   "source": [
    "%%file WordCount.java\n",
    "// dit schrijft onderstaande cell naar een file in de huidige directory\n",
    "import java.io.IOException;\n",
    "import java.util.StringTokenizer;\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "\n",
    "public class WordCount {\n",
    "    \n",
    "    // Mapper -> classes tussen <> zijn de classen van de (input key, input_value, output_key, output_value)\n",
    "    public static class WCMapper extends Mapper<Object, Text, Text, IntWritable>{\n",
    "        \n",
    "        // hier komt de mapfunctie in\n",
    "        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {\n",
    "            // map functie leest lijn per lijn\n",
    "            // lijn splitsen in woorden\n",
    "            // hello world\n",
    "            StringTokenizer itr = new StringTokenizer(value.toString());\n",
    "            // itr = [hello, world]\n",
    "            while(itr.hasMoreTokens()){\n",
    "                // hier zitten we woord per woord te lezen\n",
    "                // stuur voor elk woord (woord, 1)\n",
    "                Text word = new Text();\n",
    "                word.set(itr.nextToken());\n",
    "                System.out.println(word.toString());\n",
    "                context.write(word, new IntWritable(1));\n",
    "                // (hello, 1)\n",
    "                // (word, 1)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "   \n",
    "    // Reducer -> classes tussen <> zijn de classen van de (input key, input_value, output_key, output_value)\n",
    "    public static class WCReducer extends Reducer<Text, IntWritable, Text, IntWritable>{\n",
    "        \n",
    "        public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n",
    "            // key = hello, values = [1, 1, 1, 1]\n",
    "            int sum = 0;\n",
    "            for (IntWritable val: values) {\n",
    "                sum += val.get();\n",
    "            }\n",
    "            System.out.println(sum);\n",
    "            context.write(key, new IntWritable(sum));\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // configure the MapReduce program\n",
    "    public static void main(String[] args) throws Exception {\n",
    "        Configuration conf = new Configuration();\n",
    "        Job job = Job.getInstance(conf, \"word count java\");\n",
    "        job.setJarByClass(WordCount.class);\n",
    "        // configure mapper\n",
    "        job.setMapperClass(WCMapper.class);\n",
    "        // configure combiner (soort van reducer die draait op mapping node voor performantie)\n",
    "        job.setCombinerClass(WCReducer.class);\n",
    "        // configure reducer\n",
    "        job.setReducerClass(WCReducer.class);\n",
    "        // set output key-value classes\n",
    "        job.setOutputKeyClass(Text.class);\n",
    "        job.setOutputValueClass(IntWritable.class);\n",
    "        // set input file (first argument passed to the program)\n",
    "        FileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "        // set output file  (second argument passed to the program)\n",
    "        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "        // In this case, we wait for completion to get the output/logs and not stop the program to early.\n",
    "        System.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Deze code bevat drie delen:\n",
    "* De main() functie: verzorgt de configuratie van de uit te voeren taak. Geeft aan wat de Map en Reduce klassen zijn, wat de input is, hoe de output bewaard wordt ,...\n",
    "* De Map-klasse met de map() functie bevat de code voor de mapping-fase\n",
    "* De Reduce-klasse met de reduce() functie bevat de code voor de reduce-fase\n",
    "\n",
    "De laatste twee klassen zijn hier gecodeerd als geneste klassen. Deze hadden ook in aparte files geplaatst kunnen worden.\n",
    "Nu moet deze code eerst omgezet/gecompileerd worden tot een .jar file. Deze kan dan analoog uitgevoerd worden als hierboven met het voorbeeldcode.\n",
    "Deze twee stappen kunnen uitgevoerd worden door onderstaande commando's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added manifest\n",
      "adding: WordCount$WCMapper.class(in = 1686) (out= 758)(deflated 55%)\n",
      "adding: WordCount$WCReducer.class(in = 1768) (out= 767)(deflated 56%)\n",
      "adding: WordCount.class(in = 1494) (out= 830)(deflated 44%)\n",
      "2023-02-16 15:33:00,358 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2023-02-16 15:33:01,872 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n",
      "2023-02-16 15:33:01,927 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/bigdata/.staging/job_1675953405945_0003\n",
      "2023-02-16 15:33:03,173 INFO input.FileInputFormat: Total input files to process : 1\n",
      "2023-02-16 15:33:03,546 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2023-02-16 15:33:04,198 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1675953405945_0003\n",
      "2023-02-16 15:33:04,199 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-02-16 15:33:05,025 INFO conf.Configuration: found resource resource-types.xml at file:/home/bigdata/hadoop/etc/hadoop/resource-types.xml\n",
      "2023-02-16 15:33:05,091 INFO resource.ResourceUtils: Adding resource type - name = vram, units = G, type = COUNTABLE\n",
      "2023-02-16 15:33:05,353 INFO impl.YarnClientImpl: Submitted application application_1675953405945_0003\n",
      "2023-02-16 15:33:05,420 INFO mapreduce.Job: The url to track the job: http://bigdata-VirtualBox:8088/proxy/application_1675953405945_0003/\n",
      "2023-02-16 15:33:05,421 INFO mapreduce.Job: Running job: job_1675953405945_0003\n",
      "2023-02-16 15:33:30,322 INFO mapreduce.Job: Job job_1675953405945_0003 running in uber mode : false\n",
      "2023-02-16 15:33:30,325 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-02-16 15:33:48,417 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-02-16 15:34:01,675 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-02-16 15:34:02,731 INFO mapreduce.Job: Job job_1675953405945_0003 completed successfully\n",
      "2023-02-16 15:34:03,315 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=177\n",
      "\t\tFILE: Number of bytes written=531885\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=223\n",
      "\t\tHDFS: Number of bytes written=111\n",
      "\t\tHDFS: Number of read operations=8\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=15421\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=10426\n",
      "\t\tTotal time spent by all map tasks (ms)=15421\n",
      "\t\tTotal time spent by all reduce tasks (ms)=10426\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=15421\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=10426\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=15791104\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=10676224\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5\n",
      "\t\tMap output records=18\n",
      "\t\tMap output bytes=176\n",
      "\t\tMap output materialized bytes=177\n",
      "\t\tInput split bytes=119\n",
      "\t\tCombine input records=18\n",
      "\t\tCombine output records=15\n",
      "\t\tReduce input groups=15\n",
      "\t\tReduce shuffle bytes=177\n",
      "\t\tReduce input records=15\n",
      "\t\tReduce output records=15\n",
      "\t\tSpilled Records=30\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=179\n",
      "\t\tCPU time spent (ms)=2560\n",
      "\t\tPhysical memory (bytes) snapshot=432132096\n",
      "\t\tVirtual memory (bytes) snapshot=5298192384\n",
      "\t\tTotal committed heap usage (bytes)=360452096\n",
      "\t\tPeak Map Physical memory (bytes)=266862592\n",
      "\t\tPeak Map Virtual memory (bytes)=2644955136\n",
      "\t\tPeak Reduce Physical memory (bytes)=165269504\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2653237248\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=104\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=111\n"
     ]
    }
   ],
   "source": [
    "# compileren tot .jar\n",
    "!javac -d . WordCount.java\n",
    "!jar cvf wordcounter.jar *.class\n",
    "# execute\n",
    "!hadoop jar wordcounter.jar WordCount MapReduce/input.txt MapReduce/output_java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Oefening\n",
    "\n",
    "Kopieer nu bovenstaand java programma en pas het aan zodat het het aantal worden telt dat begint met elke letter van het alfabet (zet eerst de woorden om naar kleine letters).\n",
    "Hiervoor moet je dus het resultaat van de itr.nextToken() eerst gaan omvormen om er enkel de eerste letter eruit te halen en deze om te zetten naar lowercase).\n",
    "De nodige functies om dit resultaat kun je [hier](https://docs.oracle.com/javase/7/docs/api/java/lang/String.html) en [hier](https://docs.oracle.com/javase/8/docs/api/java/lang/Character.html) vinden.\n",
    "Indien dit gelukt is zorg er ook voor dat je geen entries toevoegt indien het eerste character geen letter is.\n",
    "Compileer dit programma naar een .jar en voer het uit.\n",
    "Controleer het resultaat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting StartLetter.java\n"
     ]
    }
   ],
   "source": [
    "%%file StartLetter.java\n",
    "import java.io.IOException;\n",
    "import java.util.StringTokenizer;\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "\n",
    "public class StartLetter {\n",
    "    \n",
    "    // Mapper -> classes tussen <> zijn de classen van de (input key, input_value, output_key, output_value)\n",
    "    public static class WCMapper extends Mapper<Object, Text, Text, IntWritable>{\n",
    "        \n",
    "        // hier komt de mapfunctie in\n",
    "        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {\n",
    "            // map functie leest lijn per lijn\n",
    "            // lijn splitsen in woorden\n",
    "            // hello world\n",
    "            StringTokenizer itr = new StringTokenizer(value.toString());\n",
    "            // itr = [hello, world]\n",
    "            while(itr.hasMoreTokens()){\n",
    "                // hello -> (h, 1)\n",
    "                // worlds -> (w, 1)\n",
    "                String woord = itr.nextToken();\n",
    "                woord = woord.toLowerCase();\n",
    "                char firstLetter = woord.charAt(0);\n",
    "                if(Character.isLetter(firstLetter)){\n",
    "                    Text word = new Text();\n",
    "                    word.set(Character.toString(firstLetter));\n",
    "                    context.write(word, new IntWritable(1));\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "   \n",
    "    // Reducer -> classes tussen <> zijn de classen van de (input key, input_value, output_key, output_value)\n",
    "    public static class WCReducer extends Reducer<Text, IntWritable, Text, IntWritable>{\n",
    "        \n",
    "        public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n",
    "            // key = hello, values = [1, 1, 1, 1]\n",
    "            int sum = 0;\n",
    "            for (IntWritable val: values) {\n",
    "                sum += val.get();\n",
    "            }\n",
    "            System.out.println(sum);\n",
    "            context.write(key, new IntWritable(sum));\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // configure the MapReduce program\n",
    "    public static void main(String[] args) throws Exception {\n",
    "        Configuration conf = new Configuration();\n",
    "        Job job = Job.getInstance(conf, \"start letter java\");\n",
    "        job.setJarByClass(WordCount.class);\n",
    "        // configure mapper\n",
    "        job.setMapperClass(WCMapper.class);\n",
    "        // configure combiner (soort van reducer die draait op mapping node voor performantie)\n",
    "        job.setCombinerClass(WCReducer.class);\n",
    "        // configure reducer\n",
    "        job.setReducerClass(WCReducer.class);\n",
    "        // set output key-value classes\n",
    "        job.setOutputKeyClass(Text.class);\n",
    "        job.setOutputValueClass(IntWritable.class);\n",
    "        // set input file (first argument passed to the program)\n",
    "        FileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "        // set output file  (second argument passed to the program)\n",
    "        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "        // In this case, we wait for completion to get the output/logs and not stop the program to early.\n",
    "        System.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added manifest\n",
      "adding: StartLetter$WCMapper.class(in = 1778) (out= 804)(deflated 54%)\n",
      "adding: StartLetter$WCReducer.class(in = 1774) (out= 766)(deflated 56%)\n",
      "adding: StartLetter.class(in = 1519) (out= 838)(deflated 44%)\n",
      "adding: WordCount$WCMapper.class(in = 1686) (out= 758)(deflated 55%)\n",
      "adding: WordCount$WCReducer.class(in = 1768) (out= 767)(deflated 56%)\n",
      "adding: WordCount.class(in = 1494) (out= 830)(deflated 44%)\n",
      "2023-02-16 15:48:27,746 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2023-02-16 15:48:28,481 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n",
      "2023-02-16 15:48:28,575 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/bigdata/.staging/job_1675953405945_0004\n",
      "2023-02-16 15:48:29,202 INFO input.FileInputFormat: Total input files to process : 1\n",
      "2023-02-16 15:48:29,442 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2023-02-16 15:48:29,956 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1675953405945_0004\n",
      "2023-02-16 15:48:29,956 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-02-16 15:48:30,422 INFO conf.Configuration: found resource resource-types.xml at file:/home/bigdata/hadoop/etc/hadoop/resource-types.xml\n",
      "2023-02-16 15:48:30,447 INFO resource.ResourceUtils: Adding resource type - name = vram, units = G, type = COUNTABLE\n",
      "2023-02-16 15:48:30,667 INFO impl.YarnClientImpl: Submitted application application_1675953405945_0004\n",
      "2023-02-16 15:48:30,814 INFO mapreduce.Job: The url to track the job: http://bigdata-VirtualBox:8088/proxy/application_1675953405945_0004/\n",
      "2023-02-16 15:48:30,815 INFO mapreduce.Job: Running job: job_1675953405945_0004\n",
      "2023-02-16 15:48:46,140 INFO mapreduce.Job: Job job_1675953405945_0004 running in uber mode : false\n",
      "2023-02-16 15:48:46,145 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-02-16 15:48:56,362 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-02-16 15:49:05,545 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-02-16 15:49:05,564 INFO mapreduce.Job: Job job_1675953405945_0004 completed successfully\n",
      "2023-02-16 15:49:05,819 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=78\n",
      "\t\tFILE: Number of bytes written=531717\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=223\n",
      "\t\tHDFS: Number of bytes written=36\n",
      "\t\tHDFS: Number of read operations=8\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7766\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6507\n",
      "\t\tTotal time spent by all map tasks (ms)=7766\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6507\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=7766\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6507\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=7952384\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=6663168\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5\n",
      "\t\tMap output records=17\n",
      "\t\tMap output bytes=102\n",
      "\t\tMap output materialized bytes=78\n",
      "\t\tInput split bytes=119\n",
      "\t\tCombine input records=17\n",
      "\t\tCombine output records=9\n",
      "\t\tReduce input groups=9\n",
      "\t\tReduce shuffle bytes=78\n",
      "\t\tReduce input records=9\n",
      "\t\tReduce output records=9\n",
      "\t\tSpilled Records=18\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=129\n",
      "\t\tCPU time spent (ms)=1620\n",
      "\t\tPhysical memory (bytes) snapshot=424701952\n",
      "\t\tVirtual memory (bytes) snapshot=5298384896\n",
      "\t\tTotal committed heap usage (bytes)=360452096\n",
      "\t\tPeak Map Physical memory (bytes)=262963200\n",
      "\t\tPeak Map Virtual memory (bytes)=2645266432\n",
      "\t\tPeak Reduce Physical memory (bytes)=161738752\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2653118464\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=104\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=36\n"
     ]
    }
   ],
   "source": [
    "# compileren tot .jar\n",
    "!javac -d . StartLetter.java\n",
    "!jar cvf allJavaClasses.jar *.class # let op, dit gaat wordcount classes meenemen in de jar dus met het eerste argument kan je kiezen welke main gestart wordt\n",
    "# execute voeg StartLetter toe -> start vanaf de main in deze klasse\n",
    "!hadoop jar allJavaClasses.jar StartLetter MapReduce/input.txt MapReduce/output_java_exer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Implementeren via Python\n",
    "\n",
    "Hoewel het Hadoop Ecosysteem geprogrammeerd is in Java en het dus er goed mee samenwerkt, is het niet verplicht om Java te gebruiken om MapReduce applicaties te schrijven.\n",
    "Java krijgt namelijk veel kritiek, vooral doordat er veel code nodig is om eenvoudige zaken te programmeren.\n",
    "Om andere programmeertalen te gebruiken worden er verscheidene API's aangeboden door Hadoop, namelijk\n",
    "* Hadoop Streaming\n",
    "    * Communicatie via stdin/stdout\n",
    "    * Gebruikt door hadoopy, mrjob, ...\n",
    "* Hadoop Pipes\n",
    "    * C++ interface voor Hadoop\n",
    "    * Communicatie via sockets\n",
    "    * Gebruikt door pydoop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Mrjob\n",
    "\n",
    "De eerste API die we bekijken is MrJobs.\n",
    "Deze API maakt gebruik van de Hadoop Streaming API.\n",
    "De voordelen van MrJobs zijn:\n",
    "* Uitgebreidde documentatie\n",
    "* Code kan lokaal uitgevoerd worden als test\n",
    "* Data serialisatie gebeurt automatisch (nadeel van Streaming API)\n",
    "* Werkt met Amazon Elastic MapReduce en Google Cloud Dataproc\n",
    "\n",
    "Het grootste nadeel is dat de StreamingAPI niet de volledige kracht heeft van het Hadoop ecosysteem omdat alles omgezet wordt naar strings (jsons)\n",
    "\n",
    "Installatie van deze package gebeurt als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!echo bigdata | sudo -S pip install mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcount_mrjob.py\n"
     ]
    }
   ],
   "source": [
    "%%file wordcount_mrjob.py\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MrWordCount(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        # 2 argumenten -> (key, value) , key is niet belangrijk dus _ , line is 1 lijn in het bestand\n",
    "        # deze functie wordt dus lijn per lijn opgeroepen\n",
    "        for word in line.split():\n",
    "            # yield -> return die de functie niet stopt\n",
    "            yield (word, 1)\n",
    "    \n",
    "    def reducer(self, word, counts):\n",
    "        # counts is een soort list van de aantal die we yielden in de mapper\n",
    "        yield (word, sum(counts))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MrWordCount.run()\n",
    "\n",
    "# let op: de output van deze file staat lokaal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deze code kan uitgevoerd worden door het commando hieronder.\n",
    "Let op dat dit een lokale file gebruikt voor de output.\n",
    "Indien je de output wil bewaren in het hdfs moet je dit nog uploaden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /home/bigdata/hadoop/bin...\n",
      "Found hadoop binary: /home/bigdata/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.0\n",
      "Looking for Hadoop streaming jar in /home/bigdata/hadoop...\n",
      "Found Hadoop streaming jar: /home/bigdata/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar\n",
      "Creating temp directory /tmp/wordcount_mrjob.bigdata.20230216.151356.247324\n",
      "uploading working dir files to hdfs:///user/bigdata/tmp/mrjob/wordcount_mrjob.bigdata.20230216.151356.247324/files/wd...\n",
      "Copying other local files to hdfs:///user/bigdata/tmp/mrjob/wordcount_mrjob.bigdata.20230216.151356.247324/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar4927860455492347435/] [] /tmp/streamjob3343779745894689764.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/bigdata/.staging/job_1675953405945_0005\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1675953405945_0005\n",
      "  Executing with tokens: []\n",
      "  found resource resource-types.xml at file:/home/bigdata/hadoop/etc/hadoop/resource-types.xml\n",
      "  Adding resource type - name = vram, units = G, type = COUNTABLE\n",
      "  Submitted application application_1675953405945_0005\n",
      "  The url to track the job: http://bigdata-VirtualBox:8088/proxy/application_1675953405945_0005/\n",
      "  Running job: job_1675953405945_0005\n",
      "  Job job_1675953405945_0005 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1675953405945_0005 completed successfully\n",
      "  Output directory: hdfs:///user/bigdata/tmp/mrjob/wordcount_mrjob.bigdata.20230216.151356.247324/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=156\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=141\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=218\n",
      "\t\tFILE: Number of bytes written=811950\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=368\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=141\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=101952512\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=16749568\n",
      "\t\tTotal time spent by all map tasks (ms)=99563\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=99563\n",
      "\t\tTotal time spent by all reduce tasks (ms)=16357\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=16357\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=99563\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=16357\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=7760\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=1133\n",
      "\t\tInput split bytes=212\n",
      "\t\tMap input records=5\n",
      "\t\tMap output bytes=176\n",
      "\t\tMap output materialized bytes=224\n",
      "\t\tMap output records=18\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=272330752\n",
      "\t\tPeak Map Virtual memory (bytes)=2648375296\n",
      "\t\tPeak Reduce Physical memory (bytes)=165363712\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2652884992\n",
      "\t\tPhysical memory (bytes) snapshot=704212992\n",
      "\t\tReduce input groups=15\n",
      "\t\tReduce input records=18\n",
      "\t\tReduce output records=15\n",
      "\t\tReduce shuffle bytes=224\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=36\n",
      "\t\tTotal committed heap usage (bytes)=593108992\n",
      "\t\tVirtual memory (bytes) snapshot=7947177984\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/bigdata/tmp/mrjob/wordcount_mrjob.bigdata.20230216.151356.247324/output\n",
      "Streaming final output from hdfs:///user/bigdata/tmp/mrjob/wordcount_mrjob.bigdata.20230216.151356.247324/output...\n",
      "Removing HDFS temp directory hdfs:///user/bigdata/tmp/mrjob/wordcount_mrjob.bigdata.20230216.151356.247324...\n",
      "Removing temp directory /tmp/wordcount_mrjob.bigdata.20230216.151356.247324...\n"
     ]
    }
   ],
   "source": [
    "!python wordcount_mrjob.py -r hadoop hdfs:///user/bigdata/MapReduce/input.txt > output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Pydoop\n",
    "\n",
    "Het word-count example door gebruik te maken van pydoop ziet er uit als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount_script.py\n"
     ]
    }
   ],
   "source": [
    "%%file wordcount_script.py\n",
    "\n",
    "# hier zet je beter code om alle directories weg te doen, anders krijg je vaak een fout dat je directory reeds bestaat\n",
    "\n",
    "def mapper(_, text, context):\n",
    "    for word in text.split():\n",
    "        print(word)\n",
    "        #raise Exception(\"oepsie\")\n",
    "        context.emit(word, 1)\n",
    "        \n",
    "def reducer(word, counts, context):\n",
    "    context.emit(word, sum(counts))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dit script kan uitgevoerd worden op de cluster als volgt:\n",
    "\n",
    "**Let op:** Indien dit script heel lang blijft hangen op 0% is er waarschijnlijk iets misgegaan.\n",
    "De timeout die gebruikt wordt om de applicatie af te sluiten is 10 minuten.\n",
    "De snelste manier op de applicatie af te sluiten is door gebruik te maken van het commando:\n",
    "    yarn application -kill <application-id>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-16 16:24:11,947 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-02-16 16:24:21,662 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2023-02-16 16:24:22,960 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/bigdata/.staging/job_1675953405945_0007\n",
      "2023-02-16 16:24:23,756 WARN mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2023-02-16 16:24:23,987 INFO input.FileInputFormat: Total input files to process : 1\n",
      "2023-02-16 16:24:24,327 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2023-02-16 16:24:25,234 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1675953405945_0007\n",
      "2023-02-16 16:24:25,235 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-02-16 16:24:25,753 INFO mapred.YARNRunner: Job jar is not present. Not adding any jar to the list of resources.\n",
      "2023-02-16 16:24:25,953 INFO conf.Configuration: found resource resource-types.xml at file:/home/bigdata/hadoop/etc/hadoop/resource-types.xml\n",
      "2023-02-16 16:24:26,026 INFO resource.ResourceUtils: Adding resource type - name = vram, units = G, type = COUNTABLE\n",
      "2023-02-16 16:24:26,299 INFO impl.YarnClientImpl: Submitted application application_1675953405945_0007\n",
      "2023-02-16 16:24:26,415 INFO mapreduce.Job: The url to track the job: http://bigdata-VirtualBox:8088/proxy/application_1675953405945_0007/\n",
      "2023-02-16 16:24:26,420 INFO mapreduce.Job: Running job: job_1675953405945_0007\n",
      "2023-02-16 16:24:46,216 INFO mapreduce.Job: Job job_1675953405945_0007 running in uber mode : false\n",
      "2023-02-16 16:24:46,218 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-02-16 16:25:03,970 INFO mapreduce.Job: Task Id : attempt_1675953405945_0007_m_000000_0, Status : FAILED\n",
      "Error: java.io.IOException: pipe child exception\n",
      "\tat it.crs4.pydoop.mapreduce.pipes.Application.abort(Application.java:234)\n",
      "\tat it.crs4.pydoop.mapreduce.pipes.PipesMapper.run(PipesMapper.java:128)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)\n",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.base/java.io.DataInputStream.readByte(DataInputStream.java:272)\n",
      "\tat org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:310)\n",
      "\tat org.apache.hadoop.io.WritableUtils.readVInt(WritableUtils.java:331)\n",
      "\tat it.crs4.pydoop.mapreduce.pipes.BinaryProtocol$UplinkReaderThread.run(BinaryProtocol.java:126)\n",
      "\n",
      "2023-02-16 16:25:16,309 INFO mapreduce.Job: Task Id : attempt_1675953405945_0007_m_000000_1, Status : FAILED\n",
      "Error: java.io.IOException: pipe child exception\n",
      "\tat it.crs4.pydoop.mapreduce.pipes.Application.abort(Application.java:234)\n",
      "\tat it.crs4.pydoop.mapreduce.pipes.PipesMapper.run(PipesMapper.java:128)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)\n",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.base/java.io.DataInputStream.readByte(DataInputStream.java:272)\n",
      "\tat org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:310)\n",
      "\tat org.apache.hadoop.io.WritableUtils.readVInt(WritableUtils.java:331)\n",
      "\tat it.crs4.pydoop.mapreduce.pipes.BinaryProtocol$UplinkReaderThread.run(BinaryProtocol.java:126)\n",
      "\n",
      "2023-02-16 16:25:29,620 INFO mapreduce.Job: Task Id : attempt_1675953405945_0007_m_000000_2, Status : FAILED\n",
      "Error: java.io.IOException: pipe child exception\n",
      "\tat it.crs4.pydoop.mapreduce.pipes.Application.abort(Application.java:234)\n",
      "\tat it.crs4.pydoop.mapreduce.pipes.PipesMapper.run(PipesMapper.java:128)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)\n",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.base/java.io.DataInputStream.readByte(DataInputStream.java:272)\n",
      "\tat org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:310)\n",
      "\tat org.apache.hadoop.io.WritableUtils.readVInt(WritableUtils.java:331)\n",
      "\tat it.crs4.pydoop.mapreduce.pipes.BinaryProtocol$UplinkReaderThread.run(BinaryProtocol.java:126)\n",
      "\n",
      "2023-02-16 16:25:48,938 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-02-16 16:25:50,000 INFO mapreduce.Job: Job job_1675953405945_0007 failed with state FAILED due to: Task failed task_1675953405945_0007_m_000000\n",
      "Job failed as tasks failed. failedMaps:1 failedReduces:0 killedMaps:0 killedReduces: 0\n",
      "\n",
      "2023-02-16 16:25:50,529 INFO mapreduce.Job: Counters: 10\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=4\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=4\n",
      "\t\tOther local map tasks=3\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=53287\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=53287\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=53287\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=54565888\n",
      "2023-02-16 16:25:50,556 INFO util.ExitUtil: Exiting with status 1: ExitException\n",
      "ERROR - RunCmdError:  command exited with 1 status\n"
     ]
    }
   ],
   "source": [
    "!pydoop script wordcount_script.py MapReduce/input.txt MapReduce/output_python_script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bovenstaande code werkt door gebruik te maken van een script, dit maakt het mogelijk om eenvoudige applicaties te schrijven.\n",
    "Voor complexere zaken (vooral in het geval dat er een bepaalde state moet bijgehouden worden kunnen er ook Mapper en Reducer classes aangemaakt worden waarmee het mogelijk is om de volledige [Pydoop API](https://crs4.github.io/pydoop/tutorial/mapred_api.html#api-tutorial) te gebruiken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount_pydoop.py\n"
     ]
    }
   ],
   "source": [
    "%%file wordcount_pydoop.py\n",
    "import pydoop.mapreduce.api as api\n",
    "import pydoop.mapreduce.pipes as pipes\n",
    "\n",
    "class Mapper(api.Mapper):\n",
    "    def map(self, context):\n",
    "        for w in context.value.split():\n",
    "            context.emit(w, 1)\n",
    "    \n",
    "class Reducer(api.Reducer):\n",
    "    def reduce(self, context):\n",
    "        context.emit(context.key, sum(context.values))\n",
    "        \n",
    "FACTORY = pipes.Factory(Mapper, reducer_class=Reducer)\n",
    "\n",
    "def main():\n",
    "    pipes.run_task(FACTORY)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-16 16:39:26,832 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-02-16 16:39:33,119 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2023-02-16 16:39:33,911 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/bigdata/.staging/job_1675953405945_0009\n",
      "2023-02-16 16:39:34,340 WARN mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2023-02-16 16:39:34,478 INFO input.FileInputFormat: Total input files to process : 1\n",
      "2023-02-16 16:39:34,695 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2023-02-16 16:39:35,239 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1675953405945_0009\n",
      "2023-02-16 16:39:35,240 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-02-16 16:39:35,635 INFO mapred.YARNRunner: Job jar is not present. Not adding any jar to the list of resources.\n",
      "2023-02-16 16:39:35,834 INFO conf.Configuration: found resource resource-types.xml at file:/home/bigdata/hadoop/etc/hadoop/resource-types.xml\n",
      "2023-02-16 16:39:35,860 INFO resource.ResourceUtils: Adding resource type - name = vram, units = G, type = COUNTABLE\n",
      "2023-02-16 16:39:36,077 INFO impl.YarnClientImpl: Submitted application application_1675953405945_0009\n",
      "2023-02-16 16:39:36,319 INFO mapreduce.Job: The url to track the job: http://bigdata-VirtualBox:8088/proxy/application_1675953405945_0009/\n",
      "2023-02-16 16:39:36,335 INFO mapreduce.Job: Running job: job_1675953405945_0009\n",
      "2023-02-16 16:39:54,748 INFO mapreduce.Job: Job job_1675953405945_0009 running in uber mode : false\n",
      "2023-02-16 16:39:54,749 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-02-16 16:40:05,027 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-02-16 16:40:16,230 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-02-16 16:40:17,264 INFO mapreduce.Job: Job job_1675953405945_0009 completed successfully\n",
      "2023-02-16 16:40:17,647 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=178\n",
      "\t\tFILE: Number of bytes written=538943\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=223\n",
      "\t\tHDFS: Number of bytes written=111\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8011\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8714\n",
      "\t\tTotal time spent by all map tasks (ms)=8011\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8714\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8011\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8714\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8203264\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=8923136\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=0\n",
      "\t\tMap output records=18\n",
      "\t\tMap output bytes=482\n",
      "\t\tMap output materialized bytes=170\n",
      "\t\tInput split bytes=119\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=15\n",
      "\t\tReduce shuffle bytes=170\n",
      "\t\tReduce input records=18\n",
      "\t\tReduce output records=15\n",
      "\t\tSpilled Records=36\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=133\n",
      "\t\tCPU time spent (ms)=1830\n",
      "\t\tPhysical memory (bytes) snapshot=435613696\n",
      "\t\tVirtual memory (bytes) snapshot=5301092352\n",
      "\t\tTotal committed heap usage (bytes)=360452096\n",
      "\t\tPeak Map Physical memory (bytes)=270188544\n",
      "\t\tPeak Map Virtual memory (bytes)=2648027136\n",
      "\t\tPeak Reduce Physical memory (bytes)=165425152\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2653065216\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=111\n",
      "INFO:PydoopSubmitter:Done\n"
     ]
    }
   ],
   "source": [
    "!pydoop submit --upload-file-to-cache wordcount_pydoop.py wordcount_pydoop MapReduce/input.txt MapReduce/output_python2 --entry-point main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcount_pydoop2.py\n"
     ]
    }
   ],
   "source": [
    "%%file wordcount_pydoop2.py\n",
    "# met counters\n",
    "import pydoop.mapreduce.api as api\n",
    "import pydoop.mapreduce.pipes as pipes\n",
    "\n",
    "class Mapper(api.Mapper):\n",
    "    def __init__(self, context):\n",
    "        super(Mapper, self).__init__(context)\n",
    "        # constructor in python\n",
    "        context.set_status(\"constructor mapper\")\n",
    "        self.input_words = context.get_counter(\"WORDCOUNT\", \"INPUT_WORDS\")\n",
    "        \n",
    "    def map(self, context):\n",
    "        words = context.value.split()\n",
    "        for w in words:\n",
    "            context.emit(w, 1)\n",
    "        context.increment_counter(self.input_words, len(words))\n",
    "    \n",
    "class Reducer(api.Reducer):\n",
    "    def __init__(self, context):\n",
    "        super(Reducer, self).__init__(context)\n",
    "        # constructor in python\n",
    "        context.set_status(\"constructor reducer\")\n",
    "        self.output_words = context.get_counter(\"WORDCOUNT\", \"OUTPUT_WORDS\")\n",
    "        \n",
    "    def reduce(self, context):\n",
    "        context.emit(context.key, sum(context.values))\n",
    "        context.increment_counter(self.output_words, 1)\n",
    "        \n",
    "FACTORY = pipes.Factory(Mapper, reducer_class=Reducer)\n",
    "\n",
    "def main():\n",
    "    pipes.run_task(FACTORY)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-16 16:50:58,967 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-02-16 16:51:08,574 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2023-02-16 16:51:09,434 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/bigdata/.staging/job_1675953405945_0011\n",
      "2023-02-16 16:51:09,887 WARN mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2023-02-16 16:51:10,058 INFO input.FileInputFormat: Total input files to process : 1\n",
      "2023-02-16 16:51:10,287 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2023-02-16 16:51:10,829 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1675953405945_0011\n",
      "2023-02-16 16:51:10,829 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-02-16 16:51:11,148 INFO mapred.YARNRunner: Job jar is not present. Not adding any jar to the list of resources.\n",
      "2023-02-16 16:51:11,337 INFO conf.Configuration: found resource resource-types.xml at file:/home/bigdata/hadoop/etc/hadoop/resource-types.xml\n",
      "2023-02-16 16:51:11,371 INFO resource.ResourceUtils: Adding resource type - name = vram, units = G, type = COUNTABLE\n",
      "2023-02-16 16:51:11,594 INFO impl.YarnClientImpl: Submitted application application_1675953405945_0011\n",
      "2023-02-16 16:51:11,688 INFO mapreduce.Job: The url to track the job: http://bigdata-VirtualBox:8088/proxy/application_1675953405945_0011/\n",
      "2023-02-16 16:51:11,690 INFO mapreduce.Job: Running job: job_1675953405945_0011\n",
      "2023-02-16 16:51:25,092 INFO mapreduce.Job: Job job_1675953405945_0011 running in uber mode : false\n",
      "2023-02-16 16:51:25,093 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-02-16 16:51:35,388 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-02-16 16:51:46,639 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-02-16 16:51:46,687 INFO mapreduce.Job: Job job_1675953405945_0011 completed successfully\n",
      "2023-02-16 16:51:47,047 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=178\n",
      "\t\tFILE: Number of bytes written=538951\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=223\n",
      "\t\tHDFS: Number of bytes written=111\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8044\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=7846\n",
      "\t\tTotal time spent by all map tasks (ms)=8044\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7846\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8044\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=7846\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8237056\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=8034304\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=0\n",
      "\t\tMap output records=18\n",
      "\t\tMap output bytes=482\n",
      "\t\tMap output materialized bytes=170\n",
      "\t\tInput split bytes=119\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=15\n",
      "\t\tReduce shuffle bytes=170\n",
      "\t\tReduce input records=18\n",
      "\t\tReduce output records=15\n",
      "\t\tSpilled Records=36\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=91\n",
      "\t\tCPU time spent (ms)=1810\n",
      "\t\tPhysical memory (bytes) snapshot=444145664\n",
      "\t\tVirtual memory (bytes) snapshot=5301854208\n",
      "\t\tTotal committed heap usage (bytes)=360452096\n",
      "\t\tPeak Map Physical memory (bytes)=273813504\n",
      "\t\tPeak Map Virtual memory (bytes)=2649772032\n",
      "\t\tPeak Reduce Physical memory (bytes)=170332160\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2652082176\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tWORDCOUNT\n",
      "\t\tINPUT_WORDS=18\n",
      "\t\tOUTPUT_WORDS=15\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=111\n",
      "INFO:PydoopSubmitter:Done\n"
     ]
    }
   ],
   "source": [
    "!pydoop submit --upload-file-to-cache wordcount_pydoop2.py wordcount_pydoop2 MapReduce/input.txt MapReduce/output_python4 --entry-point main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Indien je een fout maakt in een mapreduce applicatie blijft deze hangen tot er een timeout is. Je kan deze afbreken door middel van het volgende commando:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yarn application -kill {application_id} # see yarn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let in het commando hierboven ook op de entry point parameter. Deze is belangrijk omdat het anders niet gaat werken**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oefening\n",
    "\n",
    "Pas nu ook bovenstaande python applicatie aan om het aantal woorden dat begint met elke letter te tellen. Let er ook hierbij op dat er geen verschil is tussen hoofd- en kleine letters.\n",
    "\n",
    "Indien dit goed gelukt is, probeer ook de applicatie aan te passen om per letter de gemiddelde lengte van de woorden te berekenen.\n",
    "Let er hierbij op dat het niet rechtstreeks mogelijk is om meerdere keren de volledige iterator met de values te gebruiken. \n",
    "Terwijl dit nodig is voor het gemiddelde te berekenen met behulp van de standaard methoden (len, sum, ...).\n",
    "Een oplossing hiervoor is om gebruik te maken van de itertools package\n",
    "\n",
    "    from itertools import tee\n",
    "    \n",
    "Meer informatie over het gebruik hiervan vind je [hier](https://www.geeksforgeeks.org/python-itertools-tee/).\n",
    "\n",
    "Daarnaast is het ook belangrijk om te beseffen dat je meerdere keys kan emitten per woord indien gewenst.\n",
    "Breid nu de applicatie uit om ook de maximum lengte van alle woorden te bepalen.\n",
    "Denk hierbij erover na hoe je het onderscheid kan maken tussen de twee soorten keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount_pydoop_oefening.py\n"
     ]
    }
   ],
   "source": [
    "%%file wordcount_pydoop_oefening.py\n",
    "import pydoop.mapreduce.api as api\n",
    "import pydoop.mapreduce.pipes as pipes\n",
    "from itertools import tee\n",
    "\n",
    "class Mapper(api.Mapper):\n",
    "    def map(self, context):\n",
    "        for w in context.value.split():\n",
    "            if len(w) >0 and w[0].isalpha():\n",
    "                context.emit(w[0].lower(), 1)\n",
    "            context.emit(\"word length\", len(w))\n",
    "    \n",
    "class Reducer(api.Reducer):\n",
    "    def reduce(self, context):\n",
    "        # let op dat je bij deze aanpak geen data van twee keys kan gebruiken\n",
    "        if context.key == \"word length\":\n",
    "            # error omdat len niet bestaat        \n",
    "            #context.emit(\"gemiddelde\", sum(context.values) / len(context.values))\n",
    "            \n",
    "            # met itertools (elementen niet bewaard in aparte rij)\n",
    "            print('met itertools')\n",
    "            it1, it2 = tee(context.values, 2)\n",
    "            som = sum(it1)\n",
    "            aantal = sum(1 for _ in it2)    # bereken de lengte omdat len() niet bestaat\n",
    "            context.emit(\"gemiddelde lengte\", som/aantal)\n",
    "            \n",
    "            # met list (minst big data manier)\n",
    "            print('met list')\n",
    "            l = list(context.values)\n",
    "            context_emit(\"gemmiddelde lengte met list\", sum(l) / len(l))\n",
    "            \n",
    "            print('met for') # manuele for loop\n",
    "            som = 0\n",
    "            aantal = 0\n",
    "            for val in context.values:\n",
    "                aantal += 1\n",
    "                som += val\n",
    "            context.emit(\"gemiddelde lengte met for\", som/aantal)\n",
    "        else:\n",
    "            context.emit(context.key, sum(context.values))\n",
    "        \n",
    "FACTORY = pipes.Factory(Mapper, reducer_class=Reducer)\n",
    "\n",
    "def main():\n",
    "    pipes.run_task(FACTORY)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-16 17:14:42,690 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-02-16 17:14:48,766 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2023-02-16 17:14:49,504 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/bigdata/.staging/job_1675953405945_0016\n",
      "2023-02-16 17:14:49,791 WARN mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2023-02-16 17:14:49,910 INFO input.FileInputFormat: Total input files to process : 1\n",
      "2023-02-16 17:14:50,092 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2023-02-16 17:14:50,523 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1675953405945_0016\n",
      "2023-02-16 17:14:50,524 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-02-16 17:14:50,780 INFO mapred.YARNRunner: Job jar is not present. Not adding any jar to the list of resources.\n",
      "2023-02-16 17:14:50,912 INFO conf.Configuration: found resource resource-types.xml at file:/home/bigdata/hadoop/etc/hadoop/resource-types.xml\n",
      "2023-02-16 17:14:50,934 INFO resource.ResourceUtils: Adding resource type - name = vram, units = G, type = COUNTABLE\n",
      "2023-02-16 17:14:51,070 INFO impl.YarnClientImpl: Submitted application application_1675953405945_0016\n",
      "2023-02-16 17:14:51,110 INFO mapreduce.Job: The url to track the job: http://bigdata-VirtualBox:8088/proxy/application_1675953405945_0016/\n",
      "2023-02-16 17:14:51,111 INFO mapreduce.Job: Running job: job_1675953405945_0016\n",
      "2023-02-16 17:15:02,432 INFO mapreduce.Job: Job job_1675953405945_0016 running in uber mode : false\n",
      "2023-02-16 17:15:02,433 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-02-16 17:15:12,584 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-02-16 17:15:20,705 INFO mapreduce.Job: Task Id : attempt_1675953405945_0016_r_000000_0, Status : FAILED\n",
      "Error: java.io.IOException: pipe child exception\n",
      "\tat it.crs4.pydoop.mapreduce.pipes.Application.abort(Application.java:234)\n",
      "\tat it.crs4.pydoop.mapreduce.pipes.PipesReducer.cleanup(PipesReducer.java:106)\n",
      "\tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:179)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:628)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:390)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)\n",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.base/java.io.DataInputStream.readByte(DataInputStream.java:272)\n",
      "\tat org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:310)\n",
      "\tat org.apache.hadoop.io.WritableUtils.readVInt(WritableUtils.java:331)\n",
      "\tat it.crs4.pydoop.mapreduce.pipes.BinaryProtocol$UplinkReaderThread.run(BinaryProtocol.java:126)\n",
      "\n",
      "2023-02-16 17:15:33,927 INFO mapreduce.Job: Task Id : attempt_1675953405945_0016_r_000000_1, Status : FAILED\n",
      "Error: java.io.IOException: pipe child exception\n",
      "\tat it.crs4.pydoop.mapreduce.pipes.Application.abort(Application.java:234)\n",
      "\tat it.crs4.pydoop.mapreduce.pipes.PipesReducer.cleanup(PipesReducer.java:106)\n",
      "\tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:179)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:628)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:390)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)\n",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.base/java.io.DataInputStream.readByte(DataInputStream.java:272)\n",
      "\tat org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:310)\n",
      "\tat org.apache.hadoop.io.WritableUtils.readVInt(WritableUtils.java:331)\n",
      "\tat it.crs4.pydoop.mapreduce.pipes.BinaryProtocol$UplinkReaderThread.run(BinaryProtocol.java:126)\n",
      "\n",
      "2023-02-16 17:15:44,080 INFO mapreduce.Job: Task Id : attempt_1675953405945_0016_r_000000_2, Status : FAILED\n",
      "Error: java.io.IOException: pipe child exception\n",
      "\tat it.crs4.pydoop.mapreduce.pipes.Application.abort(Application.java:234)\n",
      "\tat it.crs4.pydoop.mapreduce.pipes.PipesReducer.cleanup(PipesReducer.java:106)\n",
      "\tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:179)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:628)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:390)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)\n",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.base/java.io.DataInputStream.readByte(DataInputStream.java:272)\n",
      "\tat org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:310)\n",
      "\tat org.apache.hadoop.io.WritableUtils.readVInt(WritableUtils.java:331)\n",
      "\tat it.crs4.pydoop.mapreduce.pipes.BinaryProtocol$UplinkReaderThread.run(BinaryProtocol.java:126)\n",
      "\n",
      "2023-02-16 17:15:58,330 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-02-16 17:15:58,388 INFO mapreduce.Job: Job job_1675953405945_0016 failed with state FAILED due to: Task failed task_1675953405945_0016_r_000000\n",
      "Job failed as tasks failed. failedMaps:0 failedReduces:1 killedMaps:0 killedReduces: 0\n",
      "\n",
      "2023-02-16 17:15:58,830 INFO mapreduce.Job: Counters: 40\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=269571\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=223\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=4\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tFailed reduce tasks=4\n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7765\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=33655\n",
      "\t\tTotal time spent by all map tasks (ms)=7765\n",
      "\t\tTotal time spent by all reduce tasks (ms)=33655\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=7765\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=33655\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=7951360\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=34462720\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=0\n",
      "\t\tMap output records=35\n",
      "\t\tMap output bytes=1053\n",
      "\t\tMap output materialized bytes=214\n",
      "\t\tInput split bytes=119\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=35\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=46\n",
      "\t\tCPU time spent (ms)=920\n",
      "\t\tPhysical memory (bytes) snapshot=269479936\n",
      "\t\tVirtual memory (bytes) snapshot=2648219648\n",
      "\t\tTotal committed heap usage (bytes)=232656896\n",
      "\t\tPeak Map Physical memory (bytes)=269479936\n",
      "\t\tPeak Map Virtual memory (bytes)=2648219648\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "2023-02-16 17:15:58,863 INFO util.ExitUtil: Exiting with status 1: ExitException\n",
      "ERROR - RunCmdError:  command exited with 1 status\n"
     ]
    }
   ],
   "source": [
    "!pydoop submit --upload-file-to-cache wordcount_pydoop_oefening.py wordcount_pydoop_oefening MapReduce/input.txt MapReduce/output_python_oef6 --entry-point main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
