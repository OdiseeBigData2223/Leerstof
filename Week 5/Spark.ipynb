{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Spark\n",
    "\n",
    "Hoewel het MapReduce algoritme van Hadoop een aantal voordelen heeft. \n",
    "De meest beperkende eigenschap van het MapReduce algoritme is de snelheid.\n",
    "Omdat alles ingelezen wordt vanaf de harde schijf, tussenresultaten op de schijf opgeslagen worden en de finale resultaten ook wordt er tot wel 90% van de rekentijd gespendeerd in lees- of schrijfopdrachten.\n",
    "\n",
    "Spark is geintroduceerd om dit te versnellen door gebruik te maken van in-memory processing.\n",
    "Hierdoor is Spark tot 3 keer sneller op grote datasets en tot 100 keer op kleinere datasets.\n",
    "\n",
    "Het spark framework kan gebruik maken van een externe opslag-locatie voor bestanden bij te houden (zoals HDFS) en bestaat uit de volgende componenten:\n",
    "* SparkCore\n",
    "* Spark SQL\n",
    "* Spark Streaming\n",
    "* MLlib\n",
    "* SparkGraph\n",
    "\n",
    "Daarnaast zijn er ook verschillende Spark Api's voor verschillende programmeertalen zoals Python, Scala, Java, ...\n",
    "Hierdoor is het framework ook flexibeler dan het standaard MapReduce algoritme.\n",
    "Heel veel informatie over het spark framework vind je in de [documentatie](https://spark.apache.org/docs/latest/quick-start.html) en de programming guides (bovenaan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pydoop.hdfs as hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 16:55:38,490 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user/bigdata/Spark\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "localFS = hdfs.hdfs(host='')\n",
    "client = hdfs.hdfs(host='localhost', port=9000)\n",
    "\n",
    "if not client.exists('/user/bigdata/Spark'):\n",
    "    client.create_directory('/user/bigdata/Spark')\n",
    "client.set_working_directory('/user/bigdata/Spark')\n",
    "print(client.working_directory())\n",
    "\n",
    "# do some cleaning in case anything else than input is present on HDFS\n",
    "for f in client.list_directory(\".\"):\n",
    "    if not f[\"name\"].endswith(\"input.txt\"):\n",
    "        client.delete(f[\"name\"], True)\n",
    "        \n",
    "# upload input.txt\n",
    "localFS.copy(\"input.txt\", client, \"input.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Installatie\n",
    "\n",
    "Een python implementatie van Spark kan eenvoudig geinstalleerd worden door het volgende commando uit te voeren. \n",
    "Dit moet maar eenmalig gebeuren.\n",
    "Om te kijken of het reeds geinstalleerd is kan je kijken naar de versie van pyspark (indien geinstalleerd). \n",
    "Als de versie correct gereturned wordt, dan is het reeds geinstalleerd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/bigdata/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.2.1\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.17\n",
      "Branch HEAD\n",
      "Compiled by user hgao on 2022-01-20T19:26:14Z\n",
      "Revision 4f25b3f71238a00508a356591553f2dfa89f8290\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark kan op drie manieren werken:\n",
    "* Boven op MapReduce (traag)\n",
    "* Boven op Yarn\n",
    "* Via zijn eigen resource manager\n",
    "\n",
    "In deze notebook gaan we gebruik maken van Spark gebruikmakende van yarn.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkContext: geeft aan hoe de cluster/storage bereikt kan worden\n",
    "# conf: configuration van de applicatie\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor de configuratie moeten we vooral twee zaken aangeven, namelijk:\n",
    "* Naam van de applicatie (is zichtbaar in de yarn)\n",
    "* Master url. De url dat het type cluster en hoe het te bereiken aangeeft. Wij gaan vooral werken met local om te communiceren met het lokale bestandssysteem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/bigdata/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023-03-13 16:55:55,544 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-03-13 16:55:59,931 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2023-03-13 16:56:05,779 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName('test').setMaster('yarn') # dit is gewoon configuratie\n",
    "sc = SparkContext(conf=conf)  # deze sparkcontext kan gebruikt worden om rdd's aan te maken of files in te lezen\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# deze maakt intern een sparksessie aan als deze nog niet bestaat, anders gebruikt hij de reeds bestane\n",
    "# dit is belangrijk omdat er maar 1 sparksessie tegelijkertijd actief kan zijn\n",
    "# heeft intern een sparkcontext: spark.sparkContext dus de eerste twee regels zijn strikt gezien niet meer nodig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Wordcount voorbeeld\n",
    "\n",
    "Om de api van pyspark te leren kennen kan je gaan naar de [documentatie](https://spark.apache.org/docs/latest/api/python/reference/index.html).\n",
    "Een eerder stap bij stap uitleg kan je [hier](https://spark.apache.org/docs/latest/api/python/getting_started/index.html) vinden.\n",
    "\n",
    "In onderstaande code gaan we stap voor stap het wordcount-voorbeeld uitwerken.\n",
    "\n",
    "Eerst moet er een pyspark context aangemaakt worden als volgt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(value='Hello World,'),\n",
       " Row(value='hello world,'),\n",
       " Row(value='hello world,'),\n",
       " Row(value=''),\n",
       " Row(value='Dit is een voorbeeld file om het Wordcount voorbeeld te testen !')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile = spark.read.text('Spark/input.txt')\n",
    "textFile.collect() \n",
    "# deze gaat de hele dataset van de cluster lokaal brengen\n",
    "# pas hier dus mee op in het geval van grote datasets\n",
    "# je maakt ook niet meer gebruik van de rekenrkacht van de cluster als je met dit resultaat verder werkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Row(value='Hello World,')\n",
      "[Row(value='Hello World,')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 16:>                                                         (0 + 1) / 1]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(textFile.count())\n",
    "print(textFile.first())\n",
    "print(textFile.filter(textFile.value.contains('Hello')).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# splits de lijn in woorden, maak van elk woord een aparte lijn\n",
    "words = sc.textFile('Spark/input.txt').flatMap(lambda line: line.split(' ')) \n",
    "result = words.map(lambda w: (w, 1)).reduceByKey(lambda a, b: a+b)\n",
    "\n",
    "#reduce by key\n",
    "#woord 1 -> [1,4,3,2,5,1,2,5] -> (a=1, b=4) -> 1 (a)+4 (b)=5 (nieuwe a) -> (nieuwe a + b=3) = 8 (nieuwe a) -> ...\n",
    "\n",
    "result.saveAsTextFile('Spark/output.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wat gebeurt er in dit voorbeeld?**\n",
    "\n",
    "Sparkcontext om een connectie te maken met de distributed storage\n",
    "De input file wordt dan ingelezen met de textFile functie.\n",
    "Door middel van de flatMap functie wordt de tekst lijn per lijn ingelezen en gesplits in woorden. \n",
    "Dit resulteert in een RDD (Resilient Distributed Dataset.\n",
    "De .map() functie maakt een key-value pair aan voor elke keer dat het woord voorkomt.\n",
    "In een laatste fase is er een reduce stap per key die de som neemt van alle keren dat het woord voorkomt om de uiteindelijke wordcount te nemen.\n",
    "Of af te ronden wordt het resultaat opgeslagen.\n",
    "\n",
    "![spark wordcount in yarn](images/yarn_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession\n",
    "\n",
    "Nu gaan we stuk voor stuk de verschillende stappen bekijken om een pyspark applicatie te maken.\n",
    "De eerste stap is het aanmaken van een sessie (SparkSession) wat het beginpunt is voor spark applications.\n",
    "Er zijn twee manieren om een SparkSession aan te maken:\n",
    "* builder()\n",
    "* newSession()\n",
    "\n",
    "Bij het aanmaken van een session wordt er intern een SparkContext object aangemaakt. \n",
    "Dit object stelt de connectie naar een cluster voor.\n",
    "Er kan maar 1 context tegelijkertijd actief zijn.\n",
    "Als je wil connecteren met een tweede cluster moet je eerst stop() oproepen op de reeds actieve context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 16:57:05,762 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n",
      "2023-03-13 16:57:07,238 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2023-03-13 16:57:12,456 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# als je een andere sparksessie wil moet je het vorige eerst stopzetten\n",
    "spark.stop()\n",
    "\n",
    "# met master kan je spark lokaal draaien (tussen rechte haakjes staat het aantal cpu cores)\n",
    "# heel wat andere opties zijn mogelijk, bvb als er extra zaken moeten toegevoegd worden voor connectie met een database\n",
    "spark = SparkSession.builder.master('local[1]').appName('lesSpark').getOrCreate()\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "spark = SparkSession.builder.config('spark.driver.cores', 2).appName('lesSpark').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD\n",
    "\n",
    "Op basis van het SparkSession object is het dan mogelijk om RDD-objecten aan te maken.\n",
    "Een RDD is de basis dataobject binnen Spark dat in parallel op verschillende nodes binnen een cluster kan uitgevoerd worden.\n",
    "Alle dataobjecten binnen spark horen tot deze klassen en dus zijn er veel mogelijkheden om RDD's aan te maken.\n",
    "Hier haal ik er twee aan:\n",
    "* parallelize() om bestaande python objecten om te zetten naar een RDD\n",
    "* textFile() of andere read methoden om bestanden op de cluster uit te lezen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[value: string]\n",
      "Spark/input.txt MapPartitionsRDD[1215] at textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "# aanmaken van rdds\n",
    "# meest manuele manier -> maak eerst een lokaal object (lijstje van tuples) -> parallelize -> dan staat het op de cluster in ram\n",
    "dataList = [('jens', 8), ('harry', 6), ('ron', 4)]\n",
    "rdd = spark.sparkContext.parallelize(dataList)\n",
    "rdd.collect()\n",
    "\n",
    "# andere manier -> om data in te lezen\n",
    "rdd2 = spark.read.text('Spark/input.txt') # dit geeft een DataFrame terug ipv van een rdd -> gemakkelijker te manipuleren\n",
    "rdd3 = spark.sparkContext.textFile('Spark/input.txt')\n",
    "print(rdd2)\n",
    "print(rdd3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Met bovenstaande methoden hebben we twee rdd's aangemaakt. \n",
    "Op deze objecten kunnen nu verscheidene operaties uitgevoerd worden.\n",
    "Een belangrijke eigenschap van dit type objecten is dat ze steeds in parallel uitgevoerd worden.\n",
    "\n",
    "De beschikbare operaties kunnen in twee groepen verdeeld worden:\n",
    "* transformaties\n",
    "* acties\n",
    "\n",
    "[Transformaties](https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-transformations/) zijn lazy-operations waarvoor de berekening uitgesteld wordt en geven een nieuw RDD terug.\n",
    "Een aantal voorbeelden van transformaties zijn:\n",
    "* flatMap()\n",
    "* map()\n",
    "* reduceByKey()\n",
    "* filter()\n",
    "* sortByKey()\n",
    "\n",
    "[Acties](https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-actions/) zijn operaties die een berekening starten (ook van de nodige transformaties) en geven een niet RDD-object terug. \n",
    "Een aantal voorbeelden hiervan zijn:\n",
    "* count()\n",
    "* collect()\n",
    "* first()\n",
    "* max()\n",
    "* reduce()\n",
    "\n",
    "Lees nu bovenstaande links en geef de functies die nodig zijn voor de volgende vragen op te lossen. Geef ook aan of het transformaties zijn of acties:\n",
    "* Het aantal keer dat elke waarde aanwezig is in de dataset (1 functie voor wordcount uit te voeren)\n",
    "* Uitfilteren van rijen\n",
    "* Groeperen van een aantal rijen op basis van een bepaalde waarde.\n",
    "* Toevoegen van een kolom aan elke key (bvb de lengte van een woord)\n",
    "* Hoe doe je head() uit pandas op RDD's?\n",
    "* Hoe doe je de apply() uit pandas op RDD's?\n",
    "\n",
    "Maak nu een spark applicatie dat van de eerste RDD (met de studenten) telt hoeveel studenten geslaagd zijn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataList = [('jens', 8), ('harry', 6), ('ron', 4)]\n",
    "rdd.filter(lambda student: student[1] >= 5).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De applicatie voor het berekenen van een gemiddelde is iets complexer.\n",
    "Dit soort applicaties kan geschreven worden als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# omdat we niet werken met dataframes zitten we met beperkte functionaliteit\n",
    "# best in twee stappen doen met rdds\n",
    "aantal = rdd.count()\n",
    "rdd.map(lambda x: x[1]).reduce(lambda x, y: (x+y)) /aantal\n",
    "# let op: aantal niet binnen de lambda functie plaatsen -> anders bekom je onderstaande berekening\n",
    "#((8+6) / aantal + 4) / aantal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schrijf nu een mapreduce applicatie in spark om de tweede RDD van de input te verwerken en het aantal woorden van elke lengte te bekomen.\n",
    "Tussenresultaten k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(5, 3), (6, 4), (0, 1), (3, 3), (2, 3), (9, 3), (4, 1), (1, 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = rdd3.flatMap(lambda line: line.split(' ')) \n",
    "result = words.map(lambda w: (len(w), 1)).reduceByKey(lambda a, b: a+b)\n",
    "\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes\n",
    "\n",
    "Een belangrijke subklasse van RDD's zijn dataframes.\n",
    "Dit is een veel gebruikte manier om gestructureerde data voor te stellen.\n",
    "Dataframes in spark is sterk gerelateerd aan de dataframes gezien in pandas.\n",
    "Het belangrijskte verschil is dat ze verdeeld worden over de cluster en operaties op de dataframes in parallel uitgevoerd worden.\n",
    "Dataframes kunnen aangemaakt worden door gebruik te maken van de createDataFrame functie in context of ingelezen worden vanuit csv's of jsons. Ten slotte kunnen dataframes ook komen van externe bronnen zoals databases als resultaat van een sql-query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>firstname</th>\n",
       "      <th>lastname</th>\n",
       "      <th>dob</th>\n",
       "      <th>gender</th>\n",
       "      <th>budget</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HarryHarryHarryHarryHarryHarry</td>\n",
       "      <td>Potter</td>\n",
       "      <td>1980-07-31</td>\n",
       "      <td>M</td>\n",
       "      <td>100000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ronald</td>\n",
       "      <td>Wemel</td>\n",
       "      <td>1980-04-01</td>\n",
       "      <td>M</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hermelijn</td>\n",
       "      <td>Griffel</td>\n",
       "      <td>1979-09-19</td>\n",
       "      <td>F</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        firstname lastname         dob gender     budget\n",
       "0  HarryHarryHarryHarryHarryHarry   Potter  1980-07-31      M  100000000\n",
       "1                          Ronald    Wemel  1980-04-01      M         10\n",
       "2                       Hermelijn  Griffel  1979-09-19      F       4000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+----------+------+---------+\n",
      "|firstname                     |lastname|dob       |gender|budget   |\n",
      "+------------------------------+--------+----------+------+---------+\n",
      "|HarryHarryHarryHarryHarryHarry|Potter  |1980-07-31|M     |100000000|\n",
      "|Ronald                        |Wemel   |1980-04-01|M     |10       |\n",
      "|Hermelijn                     |Griffel |1979-09-19|F     |4000     |\n",
      "+------------------------------+--------+----------+------+---------+\n",
      "\n",
      "-RECORD 0-------------------------\n",
      " firstname | HarryHarryHarryHa... \n",
      " lastname  | Potter               \n",
      " dob       | 1980-07-31           \n",
      " gender    | M                    \n",
      " budget    | 100000000            \n",
      "-RECORD 1-------------------------\n",
      " firstname | Ronald               \n",
      " lastname  | Wemel                \n",
      " dob       | 1980-04-01           \n",
      " gender    | M                    \n",
      " budget    | 10                   \n",
      "-RECORD 2-------------------------\n",
      " firstname | Hermelijn            \n",
      " lastname  | Griffel              \n",
      " dob       | 1979-09-19           \n",
      " gender    | F                    \n",
      " budget    | 4000                 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------+----------+------+-------------------+\n",
      "|summary|           firstname|lastname|       dob|gender|             budget|\n",
      "+-------+--------------------+--------+----------+------+-------------------+\n",
      "|  count|                   3|       3|         3|     3|                  3|\n",
      "|   mean|                null|    null|      null|  null|         3.333467E7|\n",
      "| stddev|                null|    null|      null|  null|5.773386936614157E7|\n",
      "|    min|HarryHarryHarryHa...| Griffel|1979-09-19|     F|                 10|\n",
      "|    max|              Ronald|   Wemel|1980-07-31|     M|          100000000|\n",
      "+-------+--------------------+--------+----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('HarryHarryHarryHarryHarryHarry', 'Potter','1980-07-31','M',100000000),\n",
    "  ('Ronald','Wemel','1980-04-01','M',10),\n",
    "  ('Hermelijn','Griffel','1979-09-19','F',4000)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"dob\",\"gender\",\"budget\"]\n",
    "\n",
    "import pandas as pd\n",
    "display(pd.DataFrame(data, columns=columns))\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "#df.show(2)\n",
    "df.show(truncate=False) # print tekst kolommen volledig uit\n",
    "df.show(vertical=True) # print kolommen verticaal/onder elkaar uit. is leesbaarder als er veel kolommen zijn\n",
    "\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark SQL\n",
    "\n",
    "Bovenstaande datastructuren (RDD's en Dataframes) zijn een onderdeel van het Pyspark sql module.\n",
    "De Spark API heeft een hele reeks methoden en functies om deze in te laden, uit te lezen en te manipuleren.\n",
    "Daarnaast maakt deze module het ook mogelijk om SQL-queries uit te voeren op dataframes.\n",
    "Om SQL-queries uit te voeren op dataframes moet er eerst een view gemaakt worden in het dataframe met de functie createOrReplaceTempView(\"view_name\")\n",
    "\n",
    "Daarna kan je gebruik maken van de .sql() functie om allerhande sql queries uit te voeren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------+------+---------+\n",
      "|           firstname|lastname|       dob|gender|   budget|\n",
      "+--------------------+--------+----------+------+---------+\n",
      "|HarryHarryHarryHa...|  Potter|1980-07-31|     M|100000000|\n",
      "|              Ronald|   Wemel|1980-04-01|     M|       10|\n",
      "+--------------------+--------+----------+------+---------+\n",
      "\n",
      "+--------------------+--------+----------+------+---------+\n",
      "|           firstname|lastname|       dob|gender|   budget|\n",
      "+--------------------+--------+----------+------+---------+\n",
      "|HarryHarryHarryHa...|  Potter|1980-07-31|     M|100000000|\n",
      "|              Ronald|   Wemel|1980-04-01|     M|       10|\n",
      "+--------------------+--------+----------+------+---------+\n",
      "\n",
      "+------+--------+\n",
      "|gender|count(1)|\n",
      "+------+--------+\n",
      "|     F|       1|\n",
      "|     M|       2|\n",
      "+------+--------+\n",
      "\n",
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|     F|    1|\n",
      "|     M|    2|\n",
      "+------+-----+\n",
      "\n",
      "+--------------------+--------+\n",
      "|           firstname|lastname|\n",
      "+--------------------+--------+\n",
      "|HarryHarryHarryHa...|  Potter|\n",
      "|              Ronald|   Wemel|\n",
      "|           Hermelijn| Griffel|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# je kan sql queries uitvoeren op dataframes\n",
    "# stap 1: zeg dat dit dataframe een tabel is met de naam ...\n",
    "df.createOrReplaceTempView('harry')\n",
    "# stap 2: voer queries\n",
    "spark.sql('select * from harry where gender=\"M\"').show()\n",
    "df.filter(df.gender == 'M').show()\n",
    "\n",
    "\n",
    "spark.sql('select gender, count(*) from harry group by gender').show()\n",
    "df.groupby('gender').count().show()\n",
    "\n",
    "\n",
    "df.select('firstname', 'lastname').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buiten de functionaliteit om SQL queries uit te voeren is ook het lezen en schrijven van allerhande dataformaten een belangrijk onderdeel van de pyspark sql module.\n",
    "Meer informatie hierover kun je [hier](https://spark.apache.org/docs/latest/sql-data-sources.html) vinden in de documentatie.\n",
    "In essentie ziet de code er uit als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option('delimiter', ';').option('header', True).csv('path_to_csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De opties die hierbij gekozen kunnen worden kun je vinden in de documentatie.\n",
    "\n",
    "Daarnaast zijn er ook functionaliteiten om data uit te lezen speciaal voor Machine Learning zoals libsvm en image-directories maar die worden later getoond."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oefening\n",
    "\n",
    "Net zoals RDD kunnen er een aantal operaties uitgevoerd worden op deze dataframes.\n",
    "Een volledige lijst met alle operaties kan je [hier](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html) vinden.\n",
    "Zoek de functies die gebruikt moeten worden om de volgende zaken uit te voeren:\n",
    "* Groepeer volgens een bepaalde sleutel\n",
    "* Krijg een lijst met alle kolomnamen\n",
    "* Filter rijen uit\n",
    "* Verwijder null-values in de dataset via rijen\n",
    "* Verwijder null-values door kolommen te verwijderen\n",
    "* Bereken een dataframe met statistieken van het dataframe\n",
    "* Krijg een dataframe met alle nan waarden\n",
    "* Hoe krijg je informatie zoals .info()\n",
    "* Hoe werkt het groeperen van informatie op basis van een key/kolom\n",
    "\n",
    "Probeer deze ook uit op bovenstaand aangemaakt dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- budget: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[gender: string, count: bigint]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupby\n",
    "# .columns\n",
    "# filter\n",
    "# dropna()\n",
    "# drop (eerst checken )\n",
    "# describe\n",
    "# isnan() of isnull()\n",
    "# .schema of printSchema\n",
    "df.printSchema()\n",
    "# groupby -> dit resulteert in iets dat geen dataframe\n",
    "# -> er is nog een aggregatie nodig (group -> 1 rij) om een dataframe te bekomen\n",
    "df.groupby('gender').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lees daarna volgende [link](https://sparkbyexamples.com/pyspark/pyspark-aggregate-functions/) om een idee te krijgen over hoe verschillende functies uit te voeren op deze dataframes.\n",
    "Werk nu de volgende oefening uit en maak hiervoor een spark applicatie:\n",
    "* Download de iris dataset in sklearn\n",
    "* Bewaar deze dataset als csv en upload de file naar het hdfs\n",
    "* Schrijf de code om de csv uit te lezen en om te zetten naar een dataframe\n",
    "* Print het dataschema uit voor het dataframe, hoeveel kolommen zijn er aanwezig in het dataframe.\n",
    "\n",
    "**Extra vragen week 5**\n",
    "* Bereken het minimum en maximum van de 'sepal width (cm)' en 'petal width (cm)' kolom.\n",
    "* Hernoem de target kolom naar label\n",
    "* Hernoem de labels 0 naar Soort 0, labels 1 naar Soort 1 en labels 2 naar Soort 2\n",
    "* Voer normalisatie van de eerste 4 kolommen uit (het gemiddelde ervan aftrekken en delen door de standaardafwijking)\n",
    "* Controleer de voorgaande stap door opnieuw het gemiddelde en de standaardafwijking te berekenen. Deze moeten respectievelijk 0 en 1 zijn.\n",
    "* Bereken de oppervlakte van sepal door de lengte en breedte ervan te vermenigvuldigen. Noem deze nieuwe kolom sepal area. Doe dit ook voor de petal.\n",
    "* Groepeer nu de rijen op de label kolom. Bereken per groep het gemiddelde van elke kolom. Is er een verschil tussen de verschillende klassen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "df = load_iris(as_frame=True).frame\n",
    "df.to_csv(\"input.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not client.exists('/user/bigdata/Spark/demo'):\n",
    "    client.create_directory('/user/bigdata/Spark/demo')\n",
    "client.set_working_directory('/user/bigdata/Spark/demo')\n",
    "\n",
    "# do some cleaning in case anything else than input is present on HDFS\n",
    "for f in client.list_directory(\".\"):\n",
    "    if not f[\"name\"].endswith(\"input.csv\"):\n",
    "        client.delete(f[\"name\"], True)\n",
    "        \n",
    "# upload input.txt\n",
    "localFS.copy(\"input.csv\", client, \"input.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- sepal length (cm): string (nullable = true)\n",
      " |-- sepal width (cm): string (nullable = true)\n",
      " |-- petal length (cm): string (nullable = true)\n",
      " |-- petal width (cm): string (nullable = true)\n",
      " |-- target: string (nullable = true)\n",
      "\n",
      "[Row(summary='count', sepal length (cm)='150', sepal width (cm)='150', petal length (cm)='150', petal width (cm)='150', target='150'), Row(summary='mean', sepal length (cm)='5.843333333333335', sepal width (cm)='3.057333333333334', petal length (cm)='3.7580000000000027', petal width (cm)='1.199333333333334', target='1.0'), Row(summary='stddev', sepal length (cm)='0.8280661279778637', sepal width (cm)='0.43586628493669793', petal length (cm)='1.7652982332594662', petal width (cm)='0.7622376689603467', target='0.8192319205190406'), Row(summary='min', sepal length (cm)='4.3', sepal width (cm)='2.0', petal length (cm)='1.0', petal width (cm)='0.1', target='0'), Row(summary='max', sepal length (cm)='7.9', sepal width (cm)='4.4', petal length (cm)='6.9', petal width (cm)='2.5', target='2')]\n",
      "+-------+-----------------------+-----------------------+-----------------------+----------------------+-------+\n",
      "|summary|sepal length (cm)      |sepal width (cm)       |petal length (cm)      |petal width (cm)      |label  |\n",
      "+-------+-----------------------+-----------------------+-----------------------+----------------------+-------+\n",
      "|count  |150                    |150                    |150                    |150                   |150    |\n",
      "|mean   |-1.3869461135129768E-15|-1.5866937393601195E-15|-1.4425497833296201E-15|-5.676940399249967E-16|null   |\n",
      "|stddev |0.9999999999999991     |1.0000000000000002     |1.0000000000000002     |0.9999999999999999    |null   |\n",
      "|min    |-1.8637802962695171    |-2.4258204175780507    |-1.5623422422553501    |-1.442244824810047    |Soort 0|\n",
      "|max    |2.483698580557863      |3.0804554356886436     |1.7798692259486228     |1.7063794137079435    |Soort 2|\n",
      "+-------+-----------------------+-----------------------+-----------------------+----------------------+-------+\n",
      "\n",
      "+-------+----------------------+---------------------+----------------------+---------------------+-------------------+-------------------+\n",
      "|  label|avg(sepal length (cm))|avg(sepal width (cm))|avg(petal length (cm))|avg(petal width (cm))|    avg(sepal area)|    avg(petal area)|\n",
      "+-------+----------------------+---------------------+----------------------+---------------------+-------------------+-------------------+\n",
      "|Soort 0|   -1.0111913832028143|   0.8504137151156322|   -1.3006300899993781|  -1.2507035169668688|-0.5905351580019597| 1.6311230369521224|\n",
      "|Soort 2|    0.8992840565585511| -0.19119013379398428|    1.0162588769420204|   1.0845261266006403|0.08265526897466229|  1.137718774756325|\n",
      "|Soort 1|   0.11190732664425877|  -0.6592235813216533|    0.2843712130573534|  0.16617739036622636|0.15752193231095163|0.10049717387188459|\n",
      "+-------+----------------------+---------------------+----------------------+---------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# nuttige functies\n",
    "from pyspark.sql.functions import col, avg, sum, count\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "df = spark.read.option('delimited', ',', ).option('header', True).csv('Spark/demo/input.csv')\n",
    "df.printSchema()\n",
    "df = df.drop('_c0')\n",
    "\n",
    "# vraag 1\n",
    "stats = df.describe()\n",
    "stats.filter(col('summary').isin(['min', 'max'])).collect()\n",
    "stats = stats.collect()    # breng statistieken lokaal voor vraag 4\n",
    "print(stats)\n",
    "\n",
    "# vraag 2\n",
    "#df.withColumn('label', df.target)   # maakt een kopie van de kolom\n",
    "df = df.withColumnRenamed('target', 'label')\n",
    "\n",
    "# vraag 3\n",
    "df = df.withColumn('label', \n",
    "                   f.when(col('label') == 0, 'Soort 0')    # f. is om aan te geven dat het een pyspark functie is (zie imports bovenaan de cell)\n",
    "                   .when(col('label') == 1, 'Soort 1')\n",
    "                   .otherwise('Soort 2'))\n",
    "\n",
    "# vraag 4\n",
    "# normalisatie is (kolom - gemiddelde) / stddev\n",
    "# avg(col(c)) ipv stats[1][c] gaat niet werken omdat je geen toegang hebt tot de hele kolom\n",
    "df = df.select([col(c) if c == 'label' else ((col(c) - stats[1][c]) / stats[2][c]).alias(c) for c in df.columns])\n",
    "df.describe().show(truncate=False)\n",
    "\n",
    "# vraag 5\n",
    "df = df.withColumn('sepal area', df['sepal length (cm)'] * df['sepal width (cm)'])\n",
    "df = df.withColumn('petal area', df['petal length (cm)'] * df['petal width (cm)'])\n",
    "\n",
    "# vraag 6\n",
    "df = df.groupby('label').avg()\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shared variabelen**\n",
    "\n",
    "Variabelen met read-write acces zijn zeer inefficient om te gebruiken in een cluster met sterke parallelisatie.\n",
    "Spark bied echter twee varianten aan die wel efficient geimplementeerd kunnen worden, namelijk\n",
    "* Broadcasted variabelen\n",
    "* Accumulators\n",
    "\n",
    "Broadcasted variabelen zijn read-only variabelen, die aangemaakt worden door de driver en eenmalig verspreid worden over de nodes in plaats van voor elke job.\n",
    "Dit wordt vooral gebruikt om grote data die veelvuldig gebruikt wordt te cachen op de nodes.\n",
    "Bij het gebruik van broadcast variabelen is het belangrijk om te onthouden dat je de originele variabele niet meer mag gebruiken na het aanmaken van de broadcasted variabele omdat ze anders toch nog elke job doorgestuurd wordt.\n",
    "De belangrijkste functies om te werken met broadcasted variabelen zijn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy broadcast\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dummy broadcast'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aanmaken van broadcast variabele\n",
    "var = spark.sparkContext.broadcast('dummy broadcast')\n",
    "\n",
    "# opvragen van de value\n",
    "print(var.value)\n",
    "\n",
    "# deleten van een broadcast variabele\n",
    "var.unpersist()# tijdelijke delete -> variabele terug opgevraagd wanneer nodig\n",
    "var.destroy() # permanente delete\n",
    "\n",
    "# je kan alles broadcasten wat serializeerbaar (omzetbaar naar bytes)\n",
    "# python gebruikt pickle om iets te serializeren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accumulators**\n",
    "\n",
    "Het andere type dat aangeboden wordt zijn accumulators.\n",
    "Deze laten enkel toe dat noden iets toevoegen aan een gedeelde variabele.\n",
    "Enkel de driver kan deze variabele uitlezen.\n",
    "Dit kan bijvoorbeeld gebruikt worden om tellers of sommen bij te houden.\n",
    "Deze accumulators kunnen een naam hebben (named accumulators zijn zichtbaar in de wep api).\n",
    "De ingebouwde accumulator van Spark ondersteunt enkel numerieke accumulators.\n",
    "Het is echter mogelijk om eigen accumulators toe te voegen door over te erven van de AccumulatorParam klasse en deze twee functies te implementeren:\n",
    "* zero: De begin waarde van de accumulator\n",
    "* addInPlace: Om twee waarden samen te voegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 14:49:24,003 ERROR scheduler.DAGScheduler: Failed to update accumulator 2 (org.apache.spark.api.python.PythonAccumulatorV2) for task 0\n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)\n",
      "\tat java.base/java.io.DataOutputStream.flush(DataOutputStream.java:123)\n",
      "\tat org.apache.spark.api.python.PythonAccumulatorV2.merge(PythonRDD.scala:732)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1(DAGScheduler.scala:1575)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1$adapted(DAGScheduler.scala:1566)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1566)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1678)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2639)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "2023-03-16 14:49:24,221 ERROR scheduler.DAGScheduler: Failed to update accumulator 2 (org.apache.spark.api.python.PythonAccumulatorV2) for task 0\n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)\n",
      "\tat java.base/java.io.DataOutputStream.flush(DataOutputStream.java:123)\n",
      "\tat org.apache.spark.api.python.PythonAccumulatorV2.merge(PythonRDD.scala:732)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1(DAGScheduler.scala:1575)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1$adapted(DAGScheduler.scala:1566)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1566)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1678)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2639)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "[] 8\n",
      "[8] 6\n",
      "[8, 6] 4\n",
      "2023-03-16 14:49:24,422 ERROR scheduler.DAGScheduler: Failed to update accumulator 2 (org.apache.spark.api.python.PythonAccumulatorV2) for task 0\n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)\n",
      "\tat java.base/java.io.DataOutputStream.flush(DataOutputStream.java:123)\n",
      "\tat org.apache.spark.api.python.PythonAccumulatorV2.merge(PythonRDD.scala:732)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1(DAGScheduler.scala:1575)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1$adapted(DAGScheduler.scala:1566)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1566)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1678)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2639)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ingebouwde accumulator\n",
    "accum = spark.sparkContext.accumulator(0) # beginwaarde is 0\n",
    "rdd.foreach(lambda x: accum.add(x[1]))\n",
    "accum.value  # rdd heeft drie rijen met de waarden 8, 6, 4\n",
    "\n",
    "# eigen accumulatoren toevoegen\n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "class MaxAccumulator(AccumulatorParam):\n",
    "    def zero(self, init=0):\n",
    "        return init\n",
    "\n",
    "    def addInPlace(self, old_var, add_var):\n",
    "        if add_var > old_var:\n",
    "            return add_var\n",
    "        else:\n",
    "            return old_var\n",
    "        \n",
    "accum = spark.sparkContext.accumulator(0, MaxAccumulator()) # beginwaarde is 0\n",
    "rdd.foreach(lambda x: accum.add(x[1]))\n",
    "accum.value  \n",
    "\n",
    "class ListAccumulator(AccumulatorParam):\n",
    "    def zero(self, init=0):\n",
    "        return []\n",
    "\n",
    "    def addInPlace(self, old_var, add_var):\n",
    "        print(old_var, add_var)\n",
    "        old_var.append(add_var)\n",
    "        return old_var\n",
    "        \n",
    "accum = spark.sparkContext.accumulator([], ListAccumulator()) # beginwaarde is 0\n",
    "rdd.foreach(lambda x: accum.add(x[1]))\n",
    "accum.value  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "Door de hoge populariteit van pandas in python is er een alternatief uitgewerkt binnen de laatste versie van Spark (eind 2021) dat de pandas api integreert.\n",
    "Hierdoor kan je code schrijven die identiek is aan te werken met pandas.\n",
    "Lees [dit artikel](https://towardsdatascience.com/run-pandas-as-fast-as-spark-f5eefe780c45) om meer informatie te krijgen over de verschillen tussen de dataframes API en de pandas-on-spark API.\n",
    "De documentatie voor deze api vind je [hier](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html).\n",
    "Een belangrijk onderdeel van deze documentatie omvat de best practices:\n",
    "* Check execution plan (dmv de explain() functie)\n",
    "* Use checkpoints voor fout-tolerantie en efficientie van de planner.\n",
    " * df.spark.local_checkpoint()\n",
    "* Vermeid data shuffling (sorting) omdat hierbij data tussen nodes moet gestuurd worden wat niet efficient is.\n",
    "* Vermeid berekeningen op 1 partitie (geen parallellisatie)\n",
    "* Vermeid kolomnamen startend of eindigend op \"_\"\n",
    " * Deze worden gebruikt door interne functies van pandas/spark\n",
    "* Kolomnamen moeten uniek zijn\n",
    "* Specificeer de index kolom bij omzetten dataframe en pandas-on-spark API\n",
    "* Gebruik zoveel mogelijk van de pandas-on-spark API direct ipv standaard python functies om conflicten te vermijden\n",
    " * df.sum() werkt maar sum(df) niet\n",
    "* Vermeid operaties op meerdere dataframes want deze gebruiken een join intern en is hierdoor traag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
