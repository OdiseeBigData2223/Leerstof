{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f971e510",
   "metadata": {},
   "source": [
    "# SQL Databases on hadoop\n",
    "\n",
    "Relationele Database Management Systemen zijn traditioneel verticaal schaalbaar.\n",
    "Dit conflicteert met Big Data toepassingen waar we de voorkeur geven aan horizontale schaalbaarheid om onder andere de kosten te verlagen, de beschikbare rekenkracht of performantie en fout-tolerantie te verbeteren.\n",
    "Hierdoor kunnen de reeds gekende RDBMS niet gebruikt worden.\n",
    "\n",
    "De volgende populaire applicaties kunnen gebruikt worden om SQL te gebruiken op hadoop:\n",
    "* Apache Hive\n",
    "* Cloudera Impala\n",
    "* Presto\n",
    "* Shark\n",
    "\n",
    "Hierna gaan we een kort overzicht geven van al deze applicaties en daarna focussen we op een toepassingen van Apache Hive.\n",
    "\n",
    "## Apache Hive\n",
    "\n",
    "Deze applicatie is ontwikkeld door Facebook als **datawarehouse framework** voor interne toepassingen en is snel zeer populair geworden om queries uit te voeren op Hadoop.\n",
    "Het is belangrijk om in gedachten te houden dat hoewel Hive een **SQL-like querying omgeving** aanbiedt, dat er in de achtergrond een **MapReduce methodologie** gebruikt wordt om de database te bevragen.\n",
    "Dit wil zeggen dat de **queries gecompileerd moeten worden naar MapReduce toepassingen**.\n",
    "Hive ondersteund ook gebruikers-gedefineerde functies en laat toe om gecomprimeerde data te verwerken.\n",
    "Momenteel wordt Hive verder verbeterd en uitgebreid door HortonWorks (Cloudera) dat een nieuwe backend aan het uitwerken is (Tez Project) om de responstijd van Hive te verbeteren.\n",
    "\n",
    "Voordelen:\n",
    "* Op bijna alle Hadoop installaties standaard geinstalleerd\n",
    "* Goede tool om te proberen door minimale start-investering (gratis)\n",
    "\n",
    "Nadelen:\n",
    "* Niet meest snelle manier door overhead van MapReduce (batch processing)\n",
    "* Enkel 4 file-formats ondersteund:\n",
    " * Text, SequenceFile, ORC, RCFile\n",
    "\n",
    "## Cloudera Impala\n",
    "\n",
    "Maakt het mogelijk om interactieve SQL queries uit te voeren op HDFS en HBase. \n",
    "Impala voert **queries uit in real time** en verbeterd daardoor de performantie  door geen batch processing te gebruiken.\n",
    "Daarnaast wordt ook het **gebruik van verscheidene SQL-based bedrijfsanalyse tools mogelijk** gemaakt.\n",
    "Deze applicatie is een open source applicatie ontwikkeld door Cloudera.\n",
    "\n",
    "Voordelen:\n",
    "* Sneller dan Hive\n",
    "* Ondersteund cloud based architecture door Amazon's Elastic MapReduce\n",
    "* Is compatibel met ANSI SQL (standaard SQL standaard)\n",
    "* Integratie met business intelligence tools mogelijk\n",
    "\n",
    "Nadelen:\n",
    "* Moeilijker op te zetten\n",
    "* Volledige kracht maar beschikbaar bij gebruik van Parquet file format\n",
    "* Geen support voor YARN\n",
    "* Vereist installatie van daemons op elke node\n",
    "\n",
    "## Presto\n",
    "\n",
    "Een tweede applicatie ontwikkeld door Facebook.\n",
    "Ook deze applicatie is open source.\n",
    "Deze applicatie is geschreven in Java en heeft een groot aantal kenmerken gemeen met Impala, bijvoorbeeld:\n",
    "* Een interactieve ervaring\n",
    "* Moeiljk om op te zetten (installatie op de verscheidene nodes)\n",
    "* Vereist een specifiek file format voor data opslag (RCFile)\n",
    "\n",
    "Daarnaast biedt Presto wel compatibiliteit met de Hive meta-store en laat Presto toe om data van verscheidene bronnen te combineren.\n",
    "Het grootste verschil met Impala is dat Presto niet ondersteund wordt door veel leverancies van cloud-toepassingen, ook al maken reeds een aantal grote bedrijven er gebruik van (bijvoorbeeld AirBnb en Dropbox).\n",
    "\n",
    "## Shark\n",
    "\n",
    "Deze applicatie is ontstaan om een alternatief te bieden voor Hive met MapReduce.\n",
    "Het doel was om alle functionaliteiten van Hive te behouden maar de performantie te verbeteren.\n",
    "Deze tool is geschreven in Scala door UC Berkeley en zoals de naam doet vermoeden maakt het gebruik van **Spark**.\n",
    "Tot op een zeker punt kan Shark de performantie van Hive verbeteren maar de **schaalbaarheid van de tool** is niet zo goed als Hive.\n",
    "Dit komt omdat het **gebouwd is boven op Hive** waardoor het de complexe codebase van Hive heeft overgeerfd heeft.\n",
    "Het onderhoud en aanpassen van deze codebase zonder in te boeten op performantie is echter niet eenvoudig.\n",
    "\n",
    "## Spark SQL\n",
    "\n",
    "Dit onderdeel van Spark biedt de mogelijkheid aan om Spark Queries uit te voeren op ingeladen Dataframes.\n",
    "Omdat dit gebruik maakt van Spark biedt het veel voordelen en is de performantie beter dan tools die gebruik maken van MapReduce.\n",
    "Het grootste nadeel is echter dat deze data niet standaard opgeslagen wordt op een harde schrijf.\n",
    "Het is vrij eenvoudig om deze dataframes/tabellen op te slaan als bijvoorbeeld csv maar de relaties tussen kolommen van verschillende tabellen kan niet opgeslaan worden en vereist extra manueel werk om bij te houden.\n",
    "\n",
    "## Voorbeeld toepassing: Hive\n",
    "\n",
    "Om te beginnen moet Hive geinstalleerd worden.\n",
    "Hiervoor kan je de stappen volgen op de site van hive.\n",
    "Bij oudere versies kunnen er problemen opduiken door het gebruik van oudere java versies.\n",
    "Om deze te vermijden kies je best voor voorgecompileerde versie van **apache hive versie 4 alpha 2**.\n",
    "\n",
    "Plaats de extracte files in een folder met naam hive in je home folder.\n",
    "Voeg dan aan de .bashrc file in je home folder onderaan het volgende toe:\n",
    "\n",
    "* export HIVE_HOME=/home/bigdata/hive\n",
    "* export PATH=$PATH:$HIVE_HOME/bin\n",
    "\n",
    "Indien de volgende errors opkomen kan je de volgende zaken proberen:\n",
    "* java.lang.NoSuchMethodError: 'void com.google.common.base.Preconditions.checkArgument\n",
    "    * Verwijder guava-19.0.jar in hive/lib\n",
    "    \n",
    "Na het opnieuw inladen van de systeem variabelen (herstarten terminal of uitvoeren van het source .bashrc commando) kan je beginnen werken met Hive.\n",
    "\n",
    "Eerst moet je wat initialisaties uitvoeren door middel van het volgende commando:\n",
    "\n",
    "    schematool -dbType derby -initSchema\n",
    "\n",
    "Nu kan je de Beeline CLI starten om te HQL queries uit te voeren op de Hive server met het volgende commando:\n",
    "\n",
    "    $HIVE_HOME/bin/beeline -u jdbc:hive2://\n",
    "\n",
    "Dit commando start twee zaken op, namelijk de hiveserver en de beeline CLI voor queries uit te voeren in 1 commando.\n",
    "Dit kan gebruikt worden voor te testen en te experimenteren en niet voor in productie te werken.\n",
    "De beeline CLI interfase is een sql-like console waarin we allerhande queries kunnen schrijven.\n",
    "Een uitgebreide beschrijving van alle mogelijke queries vind je [hier](https://cwiki.apache.org/confluence/display/hive/languagemanual).\n",
    "Bijvoorbeeld kunnen we de databases bekijken met\n",
    "\n",
    "    show databases;\n",
    "    \n",
    "Maak ook een database aan waarin we we gaan werken deze les.\n",
    "Schrijf hieronder de commando's die je nodig hebt om dit te doen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbdbd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!create database odisee;\n",
    "!use odisee;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b197ba29",
   "metadata": {},
   "source": [
    "Download nu deze zip files over [werknemers](https://github.com/RobinDong/hive-examples/blob/master/employee/employees.csv.gz) en [salarissen ](https://github.com/RobinDong/hive-examples/blob/master/employee/salaries.csv.gz).\n",
    "Unzip daarna deze files en upload ze naar het hdfs.\n",
    "Schrijf hieronder de nodige code om deze files in het hdfs op te slaan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "280694ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydoop.hdfs as hdfs\n",
    "\n",
    "localFS = hdfs.hdfs(host='')\n",
    "client = hdfs.hdfs(host='localhost', port=9000)\n",
    "\n",
    "if not client.exists('/user/bigdata/SQL'):\n",
    "    client.create_directory('/user/bigdata/SQL')\n",
    "\n",
    "# do some cleaning in case anything is present\n",
    "for f in client.list_directory(\".\"):\n",
    "    client.delete(f[\"name\"], True)\n",
    "        \n",
    "# upload input.txt\n",
    "localFS.copy(\"employees.csv\", client, \"SQL/employees/employees.csv\")\n",
    "localFS.copy(\"salaries.csv\", client, \"SQL/salaries/salaries.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b724b1",
   "metadata": {},
   "source": [
    "Deze files bevatten de data die we straks gaan inlezen in het Hive Datawarehouse. \n",
    "Hiervoor moeten we echter eerst de tabellen aanmaken waarin we deze data gaan opslaan.\n",
    "Dit kan door middel van de volgende HQL commando's uit te voeren in de hive HQL.\n",
    "\n",
    "Na het maken kunnen we de data inlezen door middel van het load data commando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d802f0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "create external table employee (\n",
    "    employee_id INT,\n",
    "    birthday DATE,\n",
    "    first_name STRING,\n",
    "    family_name STRING,\n",
    "    gender CHAR(1),\n",
    "    work_day DATE)\n",
    "row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "stored as textfile;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb887d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD DATA INPATH '/user/bigdata/10_SQL/employees' overwrite into table employee;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601f8f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "create external table salary (\n",
    "    employee_id INT,\n",
    "    salary INT,\n",
    "    start_date DATE,\n",
    "    end_date DATE)\n",
    "row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "stored as textfile;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89c7656",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD DATA INPATH '/user/bigdata/10_SQL/salaries' overwrite into table salary;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256bc6fb",
   "metadata": {},
   "source": [
    "Bestudeer nu opnieuw de [HQL language](https://cwiki.apache.org/confluence/display/Hive/LanguageManual) en stel de queries op die de volgende zaken opzoeken:\n",
    "* De 10 oudste werknemers\n",
    "* Het aantal werknemers dat gestart is in 1990\n",
    "* De voor en familienaam en gemiddelde salaris van de 10 werknemers met het hoogste gemiddelde salaris. (Tip: gebruik order by ipv sort by om globale orde te bepalen in reducer)\n",
    "* Is er een gender wage gap aanwezig in dit bedrijf? Bepaal hiervoor per geslacht het gemiddelde salaris aan de hand van een group by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ac11a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c03a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98ac1a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f524dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb63acff",
   "metadata": {},
   "source": [
    "Buiten rechtstreeks te werken met de hive interface om rechtstreeks queries uit te voeren kunnen we dit ook met spark doen.\n",
    "Hieronder staan de bovenstaande commando's omgezet naar queries met spark om het binnen een spark applicatie uit te voeren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b75f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40bf520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5e8e3a19af5ceb2434683dff87da6345c3b29f7eb0a8a138558c07d014a01cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
